{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06 - Time Series (ED vitals) - LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c85d4bc0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using DATA_BASE: D:/HealthAI Project/data\n"
          ]
        }
      ],
      "source": [
        "import os, glob, pandas as pd, warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# === Project-wide dataset base (Windows path provided by user) ===\n",
        "DATA_BASE = r\"D:/HealthAI Project/data\"\n",
        "\n",
        "def _glob_rel(rel_pattern):\n",
        "    pat = os.path.join(DATA_BASE, \"**\", rel_pattern)\n",
        "    return glob.glob(pat, recursive=True)\n",
        "\n",
        "def _ci_filename_search(basenames):\n",
        "    hits = []\n",
        "    for root, _, files in os.walk(DATA_BASE):\n",
        "        lowfiles = [f.lower() for f in files]\n",
        "        for name in basenames:\n",
        "            name_low = name.lower()\n",
        "            for i,f in enumerate(lowfiles):\n",
        "                if f == name_low:\n",
        "                    hits.append(os.path.join(root, files[i]))\n",
        "    return hits\n",
        "\n",
        "def find_one_any(rel_candidates, must=False, friendly=\"\"):\n",
        "    # 1) Exact-rel glob pass\n",
        "    for rel in rel_candidates:\n",
        "        hits = _glob_rel(rel)\n",
        "        if hits:\n",
        "            print(\"[found] %s -> %s\" % (rel, hits[0]))\n",
        "            return hits[0]\n",
        "    # 2) CI basename pass\n",
        "    basenames = [os.path.basename(r) for r in rel_candidates]\n",
        "    ci_hits = _ci_filename_search(basenames)\n",
        "    if ci_hits:\n",
        "        print(\"[found-ci] one of %s -> %s\" % (basenames, ci_hits[0]))\n",
        "        return ci_hits[0]\n",
        "    if must:\n",
        "        raise FileNotFoundError(f\"Could not find: {friendly or rel_candidates} under {DATA_BASE}\")\n",
        "    return None\n",
        "\n",
        "def read_csv_auto(path):\n",
        "    comp = \"gzip\" if str(path).lower().endswith(\".gz\") else None\n",
        "    return pd.read_csv(path, compression=comp, low_memory=False)\n",
        "\n",
        "print(\"Using DATA_BASE:\", DATA_BASE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "86b39784",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[found] vitalsign.csv -> D:/HealthAI Project/data\\MIMIC IV\\vitalsign.csv\n",
            "[found] edstays.csv -> D:/HealthAI Project/data\\MIMIC IV\\edstays.csv\n",
            "Using: D:/HealthAI Project/data\\MIMIC IV\\vitalsign.csv\n",
            "Using: D:/HealthAI Project/data\\MIMIC IV\\edstays.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, numpy as np, os\n",
        "\n",
        "# assumes you already have these helpers in your env:\n",
        "#   find_one_any([...]), read_csv_auto(path)\n",
        "vs_p = find_one_any([\"vitalsign.csv\"])\n",
        "es_p = find_one_any([\"edstays.csv\"])\n",
        "\n",
        "print(\"Using:\", vs_p)\n",
        "print(\"Using:\", es_p)\n",
        "\n",
        "vs = read_csv_auto(vs_p)\n",
        "es = read_csv_auto(es_p)\n",
        "\n",
        "# -------------------------\n",
        "# normalize columns & detect keys\n",
        "# -------------------------\n",
        "vs.columns = [c.lower() for c in vs.columns]\n",
        "es.columns = [c.lower() for c in es.columns]\n",
        "\n",
        "time_candidates = [\"charttime\",\"edcharttime\",\"time\",\"chart_time\"]\n",
        "stay_candidates = [\"stay_id\",\"edstay_id\",\"ed_stay_id\"]\n",
        "time_col = next((c for c in time_candidates if c in vs.columns), None)\n",
        "stay_col = next((c for c in stay_candidates if c in vs.columns), None)\n",
        "if time_col is None or stay_col is None:\n",
        "    raise RuntimeError(f\"Could not detect time/stay columns in vitalsign: {vs.columns.tolist()}\")\n",
        "\n",
        "# -------------------------\n",
        "# standardize vital names\n",
        "# -------------------------\n",
        "canon_map = {\n",
        "    \"heart_rate\":\"heart_rate\", \"heartrate\":\"heart_rate\", \"hr\":\"heart_rate\",\n",
        "    \"sbp\":\"sbp\", \"systolic_bp\":\"sbp\", \"systolic\":\"sbp\",\n",
        "    \"dbp\":\"dbp\", \"diastolic_bp\":\"dbp\", \"diastolic\":\"dbp\",\n",
        "    \"resp_rate\":\"resp_rate\", \"respiratory_rate\":\"resp_rate\", \"rr\":\"resp_rate\",\n",
        "    \"spo2\":\"spo2\", \"o2sat\":\"spo2\", \"oxygen_saturation\":\"spo2\",\n",
        "    \"temperature\":\"temperature\", \"temp\":\"temperature\", \"temp_c\":\"temperature\", \"temp_f\":\"temp_f\",\n",
        "}\n",
        "ren = {}\n",
        "for c in list(vs.columns):\n",
        "    lc = c.lower()\n",
        "    if lc in canon_map:\n",
        "        ren[c] = canon_map[lc]\n",
        "vs = vs.rename(columns=ren)\n",
        "\n",
        "candidate_vitals = [\"heart_rate\",\"sbp\",\"dbp\",\"resp_rate\",\"spo2\",\"temperature\",\"temp_f\"]\n",
        "present_vitals = [c for c in candidate_vitals if c in vs.columns]\n",
        "\n",
        "# if no heart rate recognized, try fuzzy fallback\n",
        "if \"heart_rate\" not in present_vitals:\n",
        "    hr_alt = next((c for c in vs.columns if (\"heart\" in c and \"rate\" in c)), None)\n",
        "    if hr_alt:\n",
        "        vs = vs.rename(columns={hr_alt: \"heart_rate\"})\n",
        "        present_vitals.append(\"heart_rate\")\n",
        "if \"heart_rate\" not in present_vitals:\n",
        "    raise RuntimeError(\"No heart-rate column detected after normalization.\")\n",
        "\n",
        "# convert temp_f -> temperature (Â°C) if needed\n",
        "if \"temp_f\" in vs.columns and \"temperature\" not in vs.columns:\n",
        "    vs[\"temperature\"] = (pd.to_numeric(vs[\"temp_f\"], errors=\"coerce\") - 32.0) * (5.0/9.0)\n",
        "    vs = vs.drop(columns=[\"temp_f\"])\n",
        "    present_vitals = [c for c in present_vitals if c != \"temp_f\"] + ([\"temperature\"] if \"temperature\" not in present_vitals else [])\n",
        "\n",
        "# -------------------------\n",
        "# parse times, align to known stays\n",
        "# -------------------------\n",
        "vs[time_col] = pd.to_datetime(vs[time_col], errors=\"coerce\")\n",
        "vs = vs.dropna(subset=[time_col, stay_col])\n",
        "\n",
        "stay_col_es = \"stay_id\" if \"stay_id\" in es.columns else (\"edstay_id\" if \"edstay_id\" in es.columns else (\"ed_stay_id\" if \"ed_stay_id\" in es.columns else None))\n",
        "if stay_col_es and stay_col_es != stay_col:\n",
        "    es = es.rename(columns={stay_col_es: stay_col})\n",
        "\n",
        "valid = set(es[stay_col].unique())\n",
        "vs = vs[vs[stay_col].isin(valid)]\n",
        "\n",
        "# -------------------------\n",
        "# coerce vitals to numeric BEFORE resampling\n",
        "# -------------------------\n",
        "for c in present_vitals:\n",
        "    vs[c] = pd.to_numeric(vs[c], errors=\"coerce\")\n",
        "\n",
        "# -------------------------\n",
        "# resample each stay to 15-min grid using ONLY numeric columns\n",
        "# -------------------------\n",
        "def resample_group(g):\n",
        "    num_cols = [c for c in present_vitals if c in g.columns]\n",
        "    if not num_cols:\n",
        "        return pd.DataFrame(columns=[time_col, stay_col, \"heart_rate\"])  # empty safe frame\n",
        "    g_num = g[[time_col] + num_cols].copy()\n",
        "    g_num = g_num.set_index(time_col).sort_index()\n",
        "    r = g_num.resample(\"15min\").mean()\n",
        "    # in-stay fill (time-aware, then directional)\n",
        "    r = r.interpolate(method=\"time\").ffill().bfill()\n",
        "    r = r.reset_index()\n",
        "    return r\n",
        "\n",
        "res = []\n",
        "for sid, g in vs.groupby(stay_col, sort=False):\n",
        "    r = resample_group(g)\n",
        "    if not r.empty:\n",
        "        r[stay_col] = sid\n",
        "        r = r.rename(columns={time_col: \"time\"})\n",
        "        res.append(r)\n",
        "\n",
        "if not res:\n",
        "    raise RuntimeError(\"No resampled data produced; check input columns and data quality.\")\n",
        "df = pd.concat(res, ignore_index=True)\n",
        "\n",
        "# -------------------------\n",
        "# GLOBAL fill after concatenation (handles entire-stay-missing cases)\n",
        "# -------------------------\n",
        "vital_cols = [c for c in [\"sbp\",\"dbp\",\"resp_rate\",\"spo2\",\"temperature\",\"heart_rate\"] if c in df.columns]\n",
        "# drop rows without stay id or time\n",
        "df = df.dropna(subset=[stay_col, \"time\"])\n",
        "\n",
        "# fill remaining NaNs per column with global medians\n",
        "for c in vital_cols:\n",
        "    med = pd.to_numeric(df[c], errors=\"coerce\").median(skipna=True)\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(med)\n",
        "\n",
        "# ensure target available\n",
        "if \"heart_rate\" not in df.columns:\n",
        "    raise RuntimeError(\"Target 'heart_rate' is missing after preprocessing.\")\n",
        "df = df.dropna(subset=[\"heart_rate\"])\n",
        "\n",
        "# -------------------------\n",
        "# build windows (filter out any windows containing non-finite values)\n",
        "# -------------------------\n",
        "preferred_features = [\"sbp\", \"dbp\", \"resp_rate\", \"spo2\", \"temperature\", \"heart_rate\"]\n",
        "features = [c for c in preferred_features if c in df.columns and c != \"heart_rate\"]\n",
        "target = \"heart_rate\"\n",
        "SEQ_LEN = 8\n",
        "\n",
        "def make_windows(df, seq_len=8):\n",
        "    Xs, ys, sids = [], [], []\n",
        "    for sid, g in df.sort_values([stay_col, \"time\"]).groupby(stay_col, sort=False):\n",
        "        g = g.dropna(subset=[target])\n",
        "        cols = [c for c in features + [target] if c in g.columns]\n",
        "        if (not cols) or target not in cols:\n",
        "            continue\n",
        "        G = g[cols].to_numpy(dtype=float)\n",
        "        for i in range(len(G) - seq_len):\n",
        "            X_win = G[i:i+seq_len, :]\n",
        "            y_val = G[i+seq_len, cols.index(target)]\n",
        "            # keep only all-finite windows\n",
        "            if np.isfinite(X_win).all() and np.isfinite(y_val):\n",
        "                Xs.append(X_win)\n",
        "                ys.append(y_val)\n",
        "                sids.append(sid)\n",
        "    if len(Xs) == 0:\n",
        "        return np.empty((0, seq_len, len(features)+1)), np.empty((0,), dtype=\"float32\"), np.empty((0,))\n",
        "    return np.array(Xs), np.array(ys, dtype=\"float32\"), np.array(sids)\n",
        "\n",
        "X_all, y_all, sid_all = make_windows(df, SEQ_LEN)\n",
        "if X_all.shape[0] == 0:\n",
        "    raise RuntimeError(\"No training windows were created; not enough clean sequential data per stay.\")\n",
        "\n",
        "# -------------------------\n",
        "# split by stay\n",
        "# -------------------------\n",
        "rng = np.random.default_rng(42)\n",
        "stays = np.unique(sid_all); rng.shuffle(stays)\n",
        "n = len(stays)\n",
        "train_ids = set(stays[:int(0.7*n)])\n",
        "val_ids   = set(stays[int(0.7*n):int(0.85*n)])\n",
        "test_ids  = set(stays[int(0.85*n):])\n",
        "\n",
        "def pick(ids):\n",
        "    m = np.array([s in ids for s in sid_all])\n",
        "    return X_all[m], y_all[m]\n",
        "\n",
        "X_tr, y_tr = pick(train_ids)\n",
        "X_va, y_va = pick(val_ids)\n",
        "X_te, y_te = pick(test_ids)\n",
        "\n",
        "if X_tr.size == 0 or X_va.size == 0 or X_te.size == 0:\n",
        "    raise RuntimeError(f\"Empty split(s): train {X_tr.shape[0]}, val {X_va.shape[0]}, test {X_te.shape[0]}.\")\n",
        "\n",
        "# -------------------------\n",
        "# impute any non-finite in 3D arrays by feature medians (last safety net)\n",
        "# -------------------------\n",
        "def impute_3d_inplace(X):\n",
        "    F = X.shape[-1]\n",
        "    flat = X.reshape(-1, F)\n",
        "    med = np.nanmedian(flat, axis=0)\n",
        "    # replace non-finite with medians\n",
        "    mask = ~np.isfinite(X)\n",
        "    if mask.any():\n",
        "        idx_feat = np.where(mask)[2]\n",
        "        X[mask] = np.take(med, idx_feat)\n",
        "\n",
        "impute_3d_inplace(X_tr)\n",
        "impute_3d_inplace(X_va)\n",
        "impute_3d_inplace(X_te)\n",
        "\n",
        "# -------------------------\n",
        "# scale features\n",
        "# -------------------------\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_tr2 = X_tr.reshape(-1, X_tr.shape[-1])\n",
        "scaler.fit(X_tr2)\n",
        "X_tr = scaler.transform(X_tr2).reshape(X_tr.shape)\n",
        "X_va = scaler.transform(X_va.reshape(-1, X_va.shape[-1])).reshape(X_va.shape)\n",
        "X_te = scaler.transform(X_te.reshape(-1, X_te.shape[-1])).reshape(X_te.shape)\n",
        "\n",
        "# -------------------------\n",
        "# normalize targets\n",
        "# -------------------------\n",
        "# compute on finite entries only\n",
        "finite_mask = np.isfinite(y_tr)\n",
        "y_mean = float(y_tr[finite_mask].mean()) if finite_mask.any() else 0.0\n",
        "y_std  = float(y_tr[finite_mask].std()) if finite_mask.any() else 1.0\n",
        "if not np.isfinite(y_std) or y_std == 0.0:\n",
        "    y_std = 1.0\n",
        "\n",
        "y_trn = (y_tr - y_mean) / y_std\n",
        "y_van = (y_va - y_mean) / y_std\n",
        "y_ten = (y_te - y_mean) / y_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60674920",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | Val RMSE (bpm): 1.36\n",
            "Epoch 02 | Val RMSE (bpm): 1.27\n",
            "Epoch 03 | Val RMSE (bpm): 1.31\n",
            "Epoch 04 | Val RMSE (bpm): 1.28\n",
            "Epoch 05 | Val RMSE (bpm): 1.27\n",
            "Epoch 06 | Val RMSE (bpm): 1.27\n",
            "Epoch 07 | Val RMSE (bpm): 1.25\n",
            "Epoch 08 | Val RMSE (bpm): 1.23\n",
            "Epoch 09 | Val RMSE (bpm): 1.26\n",
            "Epoch 10 | Val RMSE (bpm): 1.23\n",
            "Test RMSE (bpm): 1.41\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# LSTM regressor\n",
        "# -------------------------\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "Xtr_t = torch.tensor(X_tr, dtype=torch.float32, device=device)\n",
        "Xva_t = torch.tensor(X_va, dtype=torch.float32, device=device)\n",
        "Xte_t = torch.tensor(X_te, dtype=torch.float32, device=device)\n",
        "ytr_t = torch.tensor(y_trn.reshape(-1, 1), dtype=torch.float32, device=device)\n",
        "yva_t = torch.tensor(y_van.reshape(-1, 1), dtype=torch.float32, device=device)\n",
        "yte_t = torch.tensor(y_ten.reshape(-1, 1), dtype=torch.float32, device=device)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(Xtr_t, ytr_t), batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(TensorDataset(Xva_t, yva_t), batch_size=128)\n",
        "test_loader  = DataLoader(TensorDataset(Xte_t, yte_t), batch_size=128)\n",
        "\n",
        "input_dim = X_tr.shape[-1]\n",
        "hidden = 64\n",
        "layers = 1\n",
        "dropout = 0.1\n",
        "\n",
        "class LSTMReg(nn.Module):\n",
        "    def __init__(self, input_dim, hidden, layers, dropout):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden, num_layers=layers,\n",
        "                            batch_first=True, dropout=dropout if layers > 1 else 0.0)\n",
        "        self.head = nn.Sequential(nn.Linear(hidden, 64), nn.ReLU(), nn.Linear(64, 1))\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        last = out[:, -1, :]\n",
        "        return self.head(last)\n",
        "\n",
        "model = LSTMReg(input_dim, hidden, layers, dropout).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "def eval_rmse(loader):\n",
        "    model.eval(); se = 0.0; n = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            pred = model(xb)\n",
        "            # ensure no NaNs creep in\n",
        "            pred = torch.nan_to_num(pred)\n",
        "            yb   = torch.nan_to_num(yb)\n",
        "            se += ((pred - yb) ** 2).sum().item()\n",
        "            n  += yb.numel()\n",
        "    return (se / max(n, 1)) ** 0.5 * float(y_std)\n",
        "\n",
        "best = float(\"inf\"); best_state = None\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for xb, yb in train_loader:\n",
        "        opt.zero_grad()\n",
        "        pred = model(xb)\n",
        "        pred = torch.nan_to_num(pred)\n",
        "        yb   = torch.nan_to_num(yb)\n",
        "        loss = loss_fn(pred, yb)\n",
        "        if not torch.isfinite(loss):\n",
        "            continue  \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        opt.step()\n",
        "    val_rmse = eval_rmse(val_loader)\n",
        "    if val_rmse < best and np.isfinite(val_rmse):\n",
        "        best = val_rmse\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "    print(f\"Epoch {epoch+1:02d} | Val RMSE (bpm): {val_rmse:.2f}\")\n",
        "\n",
        "if best_state:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "test_rmse = eval_rmse(test_loader)\n",
        "print(f\"Test RMSE (bpm): {test_rmse:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e53087b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] vitals_artifacts\\lstm_reg.pt\n",
            "[saved] vitals_artifacts\\lstm_reg_ts.pt\n",
            "[saved] vitals_artifacts\\scaler.joblib\n",
            "[saved] vitals_artifacts\\meta.json\n"
          ]
        }
      ],
      "source": [
        "# =====================\n",
        "# SAVE INFERENCE ARTIFACTS\n",
        "# =====================\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import json\n",
        "import torch\n",
        "\n",
        "ART_DIR = Path(\"./Models/Vitals_Model\")\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) Save raw PyTorch state_dict (+ training metadata)\n",
        "torch.save(\n",
        "    {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"input_dim\": int(input_dim),\n",
        "        \"hidden\": int(hidden),\n",
        "        \"layers\": int(layers),\n",
        "        \"dropout\": float(dropout),\n",
        "        \"seq_len\": int(SEQ_LEN),\n",
        "        \"features\": list(features),           # order matters\n",
        "        \"target\": str(target),\n",
        "        \"y_mean\": float(y_mean),\n",
        "        \"y_std\": float(y_std),\n",
        "    },\n",
        "    ART_DIR / \"lstm_reg.pt\",\n",
        ")\n",
        "print(f\"[saved] {ART_DIR/'lstm_reg.pt'}\")\n",
        "\n",
        "# 2) Save TorchScript (portable, no class definition needed at load)\n",
        "try:\n",
        "    model_eval = model.to(\"cpu\").eval()\n",
        "    example = torch.zeros(1, SEQ_LEN, input_dim, dtype=torch.float32)\n",
        "    traced = torch.jit.trace(model_eval, example)\n",
        "    traced.save(str(ART_DIR / \"lstm_reg_ts.pt\"))\n",
        "    print(f\"[saved] {ART_DIR/'lstm_reg_ts.pt'}\")\n",
        "except Exception as e:\n",
        "    print(f\"[warn] TorchScript export failed: {e}\")\n",
        "\n",
        "# 3) Save the feature scaler\n",
        "joblib.dump(scaler, ART_DIR / \"scaler.joblib\")\n",
        "print(f\"[saved] {ART_DIR/'scaler.joblib'}\")\n",
        "\n",
        "# 4) Save extra metadata as JSON (handy for quick checks)\n",
        "meta = {\n",
        "    \"features\": list(features),\n",
        "    \"seq_len\": int(SEQ_LEN),\n",
        "    \"target\": str(target),\n",
        "    \"y_mean\": float(y_mean),\n",
        "    \"y_std\": float(y_std),\n",
        "    \"vital_columns_used\": [c for c in [\"sbp\",\"dbp\",\"resp_rate\",\"spo2\",\"temperature\",\"heart_rate\"] if c in df.columns],\n",
        "}\n",
        "with open(ART_DIR / \"meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "print(f\"[saved] {ART_DIR/'meta.json'}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
