{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19e4532",
   "metadata": {},
   "source": [
    "# 08 - Sentiment Analysis and RAG Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054ca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built RAG index | backend=tfidf | dim=83 | n=84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\workstation\\AppData\\Local\\Temp\\ipykernel_35852\\1623637919.py:86: UserWarning: SBERT path failed: cannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' (c:\\Users\\workstation\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\util\\ssl_.py)\n",
      "  warnings.warn(f\"SBERT path failed: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os, json, warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Backend selection\n",
    "EMB_BACKEND = os.getenv(\"EMB_BACKEND\", \"auto\")  # auto|sbert|tfidf\n",
    "EMB_MODEL   = os.getenv(\"EMB_MODEL\", \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "DATA = Path(\"D:/HealthAI Project/data\")\n",
    "OUT  = Path(\"Models\") / \"RAG_Model\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# UPDATED default CSV path for your dataset\n",
    "CSV = os.getenv(\"CHATBOT_CSV\", str(DATA / \"doctor_consultation_chatbot_multilingual.csv\"))\n",
    "\n",
    "def read_csv_robust(path: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"Try common encodings and both engines. Only pass low_memory to the C engine.\"\"\"\n",
    "    path = str(path)\n",
    "    for enc in [\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin1\"]:\n",
    "        # Try C engine with low_memory (fast path)\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, engine=\"c\", low_memory=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Try python engine WITHOUT low_memory (python engine doesn't support it)\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, engine=\"python\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Last resort: python engine, ignore encoding errors; DO NOT pass low_memory here\n",
    "    return pd.read_csv(path, engine=\"python\", encoding_errors=\"ignore\")\n",
    "\n",
    "def get_corpus(df: pd.DataFrame) -> list[tuple[str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Your CSV columns:\n",
    "      - id, user_query, bot_response, intent, specialty\n",
    "    We'll index user_query (question). bot_response is the answer.\n",
    "    \"\"\"\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    q_col = cols.get(\"user_query\") or cols.get(\"question\") or list(df.columns)[0]\n",
    "    a_col = cols.get(\"bot_response\") or cols.get(\"answer\") or (list(df.columns)[1] if len(df.columns) > 1 else q_col)\n",
    "\n",
    "    items: list[tuple[str, Dict[str, Any]]] = []\n",
    "    for i, row in df.iterrows():\n",
    "        q = str(row.get(q_col, \"\")).strip()\n",
    "        a = str(row.get(a_col, \"\")).strip()\n",
    "        if not q or not a:\n",
    "            continue\n",
    "        meta = {\n",
    "            \"row\":        int(i),\n",
    "            \"id\":         row.get(cols.get(\"id\", \"id\"), i),\n",
    "            \"user_query\": q,\n",
    "            \"bot_response\": a,\n",
    "            \"intent\":     str(row.get(cols.get(\"intent\",\"intent\"), \"\")).strip(),\n",
    "            \"specialty\":  str(row.get(cols.get(\"specialty\",\"specialty\"), \"\")).strip(),\n",
    "        }\n",
    "        items.append((q, meta))\n",
    "    # de-dup on (question, answer)\n",
    "    seen = set(); dedup = []\n",
    "    for q, m in items:\n",
    "        key = (q, m[\"bot_response\"])\n",
    "        if key in seen: continue\n",
    "        seen.add(key); dedup.append((q, m))\n",
    "    return dedup\n",
    "\n",
    "def build_sbert_matrix(texts: list[str]):\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model = SentenceTransformer(EMB_MODEL)\n",
    "        X = model.encode(texts, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        return X, {\"backend\": \"sbert\", \"model\": EMB_MODEL}\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"SBERT path failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def build_tfidf_matrix(texts: list[str], dim: int = 384):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    from sklearn.preprocessing import normalize\n",
    "\n",
    "    # RELAXED settings to avoid \"After pruning, no terms remain\"\n",
    "    tf = TfidfVectorizer(min_df=1, max_df=1.0, ngram_range=(1,2), strip_accents=\"unicode\", sublinear_tf=True)\n",
    "    X = tf.fit_transform(texts)\n",
    "\n",
    "    # If features/docs are too small, skip SVD safely\n",
    "    n_docs, n_feats = X.shape\n",
    "    if n_feats <= 1 or n_docs <= 2:\n",
    "        Xr = normalize(X, norm=\"l2\").toarray()\n",
    "        cfg = {\"backend\": \"tfidf\", \"dim\": int(Xr.shape[1]), \"svd\": False}\n",
    "        # still save vectorizer for inference\n",
    "        import joblib; joblib.dump({\"vectorizer\": tf, \"svd\": None}, OUT / \"tfidf_svd.joblib\")\n",
    "        return Xr, cfg\n",
    "\n",
    "    n_comp = min(dim, n_feats - 1, n_docs - 1)\n",
    "    if n_comp < 2:\n",
    "        Xr = normalize(X, norm=\"l2\").toarray()\n",
    "        cfg = {\"backend\": \"tfidf\", \"dim\": int(Xr.shape[1]), \"svd\": False}\n",
    "        import joblib; joblib.dump({\"vectorizer\": tf, \"svd\": None}, OUT / \"tfidf_svd.joblib\")\n",
    "        return Xr, cfg\n",
    "\n",
    "    svd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "    Xr = svd.fit_transform(X)\n",
    "    Xr = normalize(Xr)\n",
    "    cfg = {\"backend\": \"tfidf\", \"dim\": int(Xr.shape[1]), \"svd\": True}\n",
    "    import joblib; joblib.dump({\"vectorizer\": tf, \"svd\": svd}, OUT / \"tfidf_svd.joblib\")\n",
    "    return Xr, cfg\n",
    "\n",
    "def save_faiss(X: np.ndarray, metas: list[Dict[str, Any]], backend_cfg: Dict[str, Any]):\n",
    "    import faiss\n",
    "    d = X.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(X.astype(\"float32\"))\n",
    "    faiss.write_index(index, str(OUT / \"index.faiss\"))\n",
    "\n",
    "    with open(OUT / \"meta.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for m in metas:\n",
    "            f.write(json.dumps(m, ensure_ascii=False) + \"\\n\")\n",
    "    with open(OUT / \"config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"dim\": d, **backend_cfg}, f, indent=2)\n",
    "\n",
    "def main():\n",
    "    df = read_csv_robust(CSV)\n",
    "    recs = get_corpus(df)\n",
    "    if not recs:\n",
    "        raise SystemExit(\"No records to index (check your CSV).\")\n",
    "    texts = [t for t, _ in recs]\n",
    "    metas = [m for _, m in recs]\n",
    "\n",
    "    X = None; info = None\n",
    "    order = [\"sbert\",\"tfidf\"] if EMB_BACKEND in (\"auto\",\"sbert\") else [\"tfidf\"]\n",
    "    for b in order:\n",
    "        if b == \"sbert\":\n",
    "            X, info = build_sbert_matrix(texts)\n",
    "            if X is not None: break\n",
    "        else:\n",
    "            X, info = build_tfidf_matrix(texts)\n",
    "            if X is not None: break\n",
    "\n",
    "    if X is None:\n",
    "        raise SystemExit(\"Failed to build embeddings with any backend.\")\n",
    "\n",
    "    save_faiss(X, metas, info)\n",
    "    print(f\"Built RAG index | backend={info.get('backend')} | dim={X.shape[1]} | n={len(texts)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebfdc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] {'accuracy': 0.88, 'precision_w': 0.8881270903010033, 'recall_w': 0.88, 'f1_w': 0.8825038035209737, 'mcc': 0.7105516893606464}\n",
      "[test] {'accuracy': 0.92, 'precision_w': 0.9189083820662768, 'recall_w': 0.92, 'f1_w': 0.918609022556391, 'mcc': 0.7906954926148669}\n",
      "Saved → Models\\sentiment\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, matthews_corrcoef\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "\n",
    "DATA = Path(\"data\") / \"patient_feedback.csv\"\n",
    "OUT  = Path(\"Models\") / \"Sentiment_Model\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED=42; TEST_SIZE=0.15; VAL_SPLIT=0.50\n",
    "\n",
    "def robust_read_csv(path: Path) -> pd.DataFrame:\n",
    "    for enc in [\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin1\"]:\n",
    "        for eng in [\"c\", \"python\"]:\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc, engine=eng, low_memory=False)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return pd.read_csv(path, engine=\"python\", encoding_errors=\"ignore\")\n",
    "\n",
    "def normalize_header(s: str) -> str:\n",
    "    s = (s if isinstance(s, str) else str(s)).replace(\"\\xa0\", \" \").strip().lower()\n",
    "    return re.sub(r\"\\s+\",\" \",s)\n",
    "\n",
    "def pick_col(df: pd.DataFrame, candidates) -> str|None:\n",
    "    m = {normalize_header(c): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        n = normalize_header(cand)\n",
    "        for k, orig in m.items():\n",
    "            if k == n or k.replace(\" \",\"\")==n.replace(\" \",\"\"):\n",
    "                return orig\n",
    "    for cand in candidates:\n",
    "        n = normalize_header(cand).replace(\" \",\"\")\n",
    "        for k, orig in m.items():\n",
    "            if n in k.replace(\" \",\"\"):\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "def load_feedback(path: Path):\n",
    "    df = robust_read_csv(path)\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype==object:\n",
    "            df[c]=df[c].astype(str).str.replace(\"\\xa0\",\" \",regex=False)\n",
    "    text_col  = pick_col(df, [\"feedback\",\"review\",\"comment\",\"text\"])\n",
    "    label_col = pick_col(df, [\"sentiment label\",\"sentiment\",\"label\"])\n",
    "    rate_col  = pick_col(df, [\"ratings\",\"rating\",\"stars\",\"score\"])\n",
    "    if text_col is None: raise ValueError(f\"No text column found. Columns: {list(df.columns)}\")\n",
    "    if label_col is None and rate_col is None: raise ValueError(\"Need a label or ratings column.\")\n",
    "    if label_col is not None:\n",
    "        lab = df[label_col].astype(str).str.lower().str.strip().replace({\n",
    "            \"neg\":\"negative\",\"-1\":\"negative\",\"negative\":\"negative\",\n",
    "            \"pos\":\"positive\",\"1\":\"positive\",\"positive\":\"positive\",\n",
    "            \"neu\":\"neutral\",\"0\":\"neutral\",\"neutral\":\"neutral\"\n",
    "        })\n",
    "        df[\"label\"]=lab.fillna(\"neutral\")\n",
    "    else:\n",
    "        def rate2label(x):\n",
    "            try: r=float(str(x).strip())\n",
    "            except: return \"neutral\"\n",
    "            if r<=2: return \"negative\"\n",
    "            if r>=4: return \"positive\"\n",
    "            return \"neutral\"\n",
    "        df[\"label\"]=df[rate_col].apply(rate2label)\n",
    "    df[text_col]=df[text_col].astype(str).str.strip()\n",
    "    df=df[[text_col,\"label\"]].rename(columns={text_col:\"text\"}).dropna()\n",
    "    df=df[df[\"text\"].str.len()>0]\n",
    "    labels=sorted(df[\"label\"].unique())\n",
    "    return df, labels\n",
    "\n",
    "def evaluate(y_true,y_pred):\n",
    "    prec,rec,f1,_=precision_recall_fscore_support(y_true,y_pred,average=\"weighted\",zero_division=0)\n",
    "    mcc=matthews_corrcoef(y_true,y_pred); acc=(y_true==y_pred).mean()\n",
    "    return {\"accuracy\": float(acc),\"precision_w\":float(prec),\"recall_w\":float(rec),\"f1_w\":float(f1),\"mcc\":float(mcc)}\n",
    "\n",
    "def main():\n",
    "    df, labels = load_feedback(DATA)\n",
    "    label2id={l:i for i,l in enumerate(labels)}\n",
    "    id2label={i:l for l,i in label2id.items()}\n",
    "    df[\"label_id\"]=df[\"label\"].map(label2id)\n",
    "\n",
    "    tr, te = train_test_split(df, test_size=TEST_SIZE, random_state=SEED, stratify=df[\"label_id\"])\n",
    "    va, ev = train_test_split(te, test_size=VAL_SPLIT, random_state=SEED, stratify=te[\"label_id\"])\n",
    "\n",
    "    Xtr, ytr = tr[\"text\"].tolist(), tr[\"label_id\"].to_numpy()\n",
    "    Xva, yva = va[\"text\"].tolist(), va[\"label_id\"].to_numpy()\n",
    "    Xev, yev = ev[\"text\"].tolist(), ev[\"label_id\"].to_numpy()\n",
    "\n",
    "    classes=np.array(sorted(np.unique(ytr)))\n",
    "    cw=compute_class_weight(class_weight=\"balanced\", classes=classes, y=ytr)\n",
    "    w={int(c):float(v) for c,v in zip(classes,cw)}\n",
    "\n",
    "    from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    # LinearSVC is fine; keeping it simple:\n",
    "    from sklearn.svm import LinearSVC\n",
    "    pipe=Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2),\n",
    "                                  min_df=2, max_df=0.98,\n",
    "                                  strip_accents=\"unicode\", sublinear_tf=True)),\n",
    "        (\"clf\", LinearSVC(class_weight=w, random_state=SEED))\n",
    "    ])\n",
    "    pipe.fit(Xtr,ytr)\n",
    "    print(\"[val]\", evaluate(yva, pipe.predict(Xva)))\n",
    "    print(\"[test]\", evaluate(yev, pipe.predict(Xev)))\n",
    "\n",
    "    OUT.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(pipe, OUT/\"sklearn_model.joblib\")\n",
    "    (OUT/\"label_map.json\").write_text(json.dumps({\"labels\": labels, \"label2id\": label2id, \"id2label\": id2label}, indent=2), encoding=\"utf-8\")\n",
    "    (OUT/\"model_adapter.json\").write_text(json.dumps({\"type\":\"sklearn\",\"path\":str(OUT/\"sklearn_model.joblib\")}), encoding=\"utf-8\")\n",
    "    print(f\"Saved → {OUT}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
