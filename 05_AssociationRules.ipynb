{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a7ef8e9",
   "metadata": {},
   "source": [
    "# 05 - Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "866fa799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, warnings, itertools\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option(\"display.max_columns\", 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 0) Config / paths\n",
    "# -----------------------------\n",
    "DATA_DIR = r\"D:/HealthAI Project/data\"   # folder already extracted\n",
    "ART = Path(\"./Models/Assoc_Models\"); ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Targets for the high-confidence search\n",
    "MIN_CONFIDENCE_TARGET = 0.70\n",
    "MIN_LIFT_TARGET = 1.50\n",
    "MAX_LEN = 3\n",
    "\n",
    "# Support grid to try (stop at the first that yields usable high-confidence rules)\n",
    "MIN_SUPPORT_GRID = (0.08, 0.06, 0.05, 0.04, 0.03, 0.02, 0.015, 0.01, 0.008, 0.006, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160826c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using diagnoses CSV: D:/HealthAI Project/data\\MIMIC IV\\diagnoses_icd.csv\n",
      "[INFO] diagnoses shape: (6364490, 5)\n",
      "[INFO] diagnoses columns: ['subject_id', 'hadm_id', 'seq_num', 'icd_code', 'icd_version'] ...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1) Find the diagnoses CSV (already extracted)\n",
    "# -----------------------------\n",
    "def find_diagnoses_file(base_dir: str) -> str:\n",
    "    patterns = [\n",
    "        r\"diagnos.*icd.*\\.csv(?:\\.gz)?$\",  # e.g., diagnoses_icd.csv(.gz)\n",
    "        r\"diagnos.*\\.csv(?:\\.gz)?$\",       # e.g., diagnoses.csv(.gz)\n",
    "    ]\n",
    "    hits = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for f in files:\n",
    "            low = f.lower()\n",
    "            if any(re.search(p, low) for p in patterns):\n",
    "                hits.append(os.path.join(root, f))\n",
    "    if not hits:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not locate a diagnoses CSV (e.g., diagnoses_icd.csv) under: {base_dir}\"\n",
    "        )\n",
    "    hits.sort(key=lambda p: len(p))  # prefer shorter (often the main one)\n",
    "    return hits[0]\n",
    "\n",
    "DIAG_PATH = find_diagnoses_file(DATA_DIR)\n",
    "print(f\"[INFO] Using diagnoses CSV: {DIAG_PATH}\")\n",
    "\n",
    "diagnoses = pd.read_csv(DIAG_PATH, low_memory=False)\n",
    "print(\"[INFO] diagnoses shape:\", diagnoses.shape)\n",
    "print(\"[INFO] diagnoses columns:\", diagnoses.columns.tolist()[:12], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534606a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) Normalize columns\n",
    "# -----------------------------\n",
    "cols = {c.lower(): c for c in diagnoses.columns}\n",
    "SUBJ_COL = cols.get(\"subject_id\", None)\n",
    "CODE_COL = cols.get(\"icd_code\", cols.get(\"icd9_code\", cols.get(\"icd10_code\", None)))\n",
    "VER_COL  = cols.get(\"icd_version\", None)\n",
    "\n",
    "if SUBJ_COL is None or CODE_COL is None:\n",
    "    raise RuntimeError(f\"Missing required columns. Found: {diagnoses.columns.tolist()}\")\n",
    "\n",
    "if VER_COL is None:\n",
    "    sample_codes = diagnoses[CODE_COL].astype(str).head(200).tolist()\n",
    "    guess_10 = sum(1 for c in sample_codes if re.match(r\"^[A-Z]\", str(c).strip(), re.I))\n",
    "    icd_version_default = 10 if guess_10 > (len(sample_codes) / 2.0) else 9\n",
    "    diagnoses[\"icd_version\"] = icd_version_default\n",
    "    VER_COL = \"icd_version\"\n",
    "\n",
    "diagnoses[\"icd_code\"] = diagnoses[CODE_COL].astype(str)\n",
    "diagnoses[\"icd_version\"] = pd.to_numeric(diagnoses[VER_COL], errors=\"coerce\").fillna(10).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc686006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) ICD -> comorbidity mapping\n",
    "# -----------------------------\n",
    "def icd_to_conditions(code, version):\n",
    "    \"\"\"Return a set of comorbidity item labels from an ICD code (9/10).\"\"\"\n",
    "    if not isinstance(code, str):\n",
    "        code = str(code)\n",
    "    code = code.strip().upper().replace('.', '')\n",
    "    out = set()\n",
    "    if not code:\n",
    "        return out\n",
    "\n",
    "    if version == 9 or (code[:1].isdigit() and len(code) >= 3):\n",
    "        # ---- ICD-9 ----\n",
    "        try:\n",
    "            num3 = float(code[:3])\n",
    "        except Exception:\n",
    "            num3 = None\n",
    "        if num3 is not None and 401 <= num3 < 406: out.add(\"Hypertension\")\n",
    "        if code.startswith(\"250\"): out.add(\"Diabetes\")\n",
    "        if code.startswith(\"278\"): out.add(\"Obesity\")\n",
    "        if code.startswith(\"272\"): out.add(\"Hyperlipidemia\")\n",
    "        if num3 is not None and 410 <= num3 < 415: out.add(\"Coronary_Artery_Disease\")\n",
    "        if code.startswith(\"428\"): out.add(\"Heart_Failure\")\n",
    "        if code.startswith(\"585\"): out.add(\"Chronic_Kidney_Disease\")\n",
    "        if (num3 is not None and (491 <= num3 < 493)) or code.startswith(\"492\") or code.startswith(\"496\"):\n",
    "            out.add(\"COPD\")\n",
    "        if code.startswith(\"493\"): out.add(\"Asthma\")\n",
    "        if code.startswith(\"2962\") or code.startswith(\"2963\") or code.startswith(\"311\"):\n",
    "            out.add(\"Depression\")\n",
    "        if num3 is not None and 280 <= num3 < 286: out.add(\"Anemia\")\n",
    "        if code.startswith(\"530\"): out.add(\"GERD\")\n",
    "        if code.startswith(\"3051\"): out.add(\"Tobacco_Use\")\n",
    "    else:\n",
    "        # ---- ICD-10 ----\n",
    "        if code.startswith((\"I10\",\"I11\",\"I12\",\"I13\",\"I15\",\"I16\",\"I14\")): out.add(\"Hypertension\")\n",
    "        if code.startswith((\"E10\",\"E11\",\"E12\",\"E13\",\"E14\")): out.add(\"Diabetes\")\n",
    "        if code.startswith(\"E66\"): out.add(\"Obesity\")\n",
    "        if code.startswith(\"E78\"): out.add(\"Hyperlipidemia\")\n",
    "        if code.startswith((\"I20\",\"I21\",\"I22\",\"I23\",\"I24\",\"I25\")): out.add(\"Coronary_Artery_Disease\")\n",
    "        if code.startswith(\"I50\"): out.add(\"Heart_Failure\")\n",
    "        if code.startswith(\"N18\"): out.add(\"Chronic_Kidney_Disease\")\n",
    "        if code.startswith(\"J44\"): out.add(\"COPD\")\n",
    "        if code.startswith(\"J45\"): out.add(\"Asthma\")\n",
    "        if code.startswith((\"F32\",\"F33\")): out.add(\"Depression\")\n",
    "        if code[:3] in {f\"D{n}\" for n in range(50,65)}: out.add(\"Anemia\")\n",
    "        if code.startswith(\"K21\"): out.add(\"GERD\")\n",
    "        if code.startswith(\"F17\") or code.startswith(\"Z720\") or code.startswith(\"Z72\"): out.add(\"Tobacco_Use\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e461f75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Patients with ≥1 mapped comorbidity: 177633\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4) Build patient-level transactions\n",
    "# -----------------------------\n",
    "baskets = {}  # subject_id -> set(items)\n",
    "for _, r in diagnoses.iterrows():\n",
    "    sid = r[SUBJ_COL]\n",
    "    conds = icd_to_conditions(r[\"icd_code\"], r[\"icd_version\"])\n",
    "    if not conds:\n",
    "        continue\n",
    "    s = baskets.get(sid, set())\n",
    "    s.update(conds)\n",
    "    baskets[sid] = s\n",
    "\n",
    "print(f\"[INFO] Patients with ≥1 mapped comorbidity: {len(baskets)}\")\n",
    "if not baskets:\n",
    "    raise RuntimeError(\"No comorbidity items were mapped from codes. Check mapping / columns.\")\n",
    "\n",
    "# Save transactions for transparency\n",
    "tx_rows = [{\"subject_id\": sid, \"items\": \", \".join(sorted(list(items)))} for sid, items in baskets.items()]\n",
    "pd.DataFrame(tx_rows).to_csv(ART/\"assoc_transactions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9780aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Basket matrix: (177633, 13) (patients x items)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5) One-hot encode transactions (boolean DataFrame)\n",
    "# -----------------------------\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "sids = list(baskets.keys())\n",
    "item_lists = [sorted(list(baskets[sid])) for sid in sids]\n",
    "mlb = MultiLabelBinarizer()\n",
    "X_bool = pd.DataFrame(mlb.fit_transform(item_lists).astype(bool),\n",
    "                      index=sids, columns=list(mlb.classes_))\n",
    "items = list(X_bool.columns)\n",
    "N = X_bool.shape[0]\n",
    "print(f\"[INFO] Basket matrix: {X_bool.shape} (patients x items)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7da3b955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using mlxtend.frequent_patterns.apriori / association_rules\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6) Mining helpers (mlxtend or fallback)\n",
    "# -----------------------------\n",
    "USE_MLXTEND = False\n",
    "try:\n",
    "    from mlxtend.frequent_patterns import apriori, association_rules\n",
    "    USE_MLXTEND = True\n",
    "    print(\"[INFO] Using mlxtend.frequent_patterns.apriori / association_rules\")\n",
    "except Exception:\n",
    "    print(\"[INFO] mlxtend not found; using fast fallback Apriori (max_len<=3)\")\n",
    "    USE_MLXTEND = False\n",
    "\n",
    "def apriori_fallback_bool(B: pd.DataFrame, min_support=0.1, max_len=3):\n",
    "    \"\"\"Compact Apriori for boolean DataFrame; returns DataFrame with columns: itemset(tuple), support.\"\"\"\n",
    "    Bv = B.values.astype(bool)\n",
    "    names = list(B.columns)\n",
    "    n, d = Bv.shape\n",
    "\n",
    "    # L1\n",
    "    L = []\n",
    "    freq = []\n",
    "    for j in range(d):\n",
    "        s = Bv[:, j].mean()\n",
    "        if s >= min_support:\n",
    "            L.append((frozenset([names[j]]), (j,)))\n",
    "            freq.append({\"itemset\": frozenset([names[j]]), \"support\": float(s)})\n",
    "\n",
    "    k = 2\n",
    "    prev = L\n",
    "    while k <= max_len and prev:\n",
    "        cand = {}\n",
    "        for i in range(len(prev)):\n",
    "            for j in range(i+1, len(prev)):\n",
    "                iset = prev[i][0] | prev[j][0]\n",
    "                if len(iset) == k:\n",
    "                    idxs = tuple(sorted(set(prev[i][1]) | set(prev[j][1])))\n",
    "                    cand[iset] = idxs\n",
    "        new_prev = []\n",
    "        for iset, idxs in cand.items():\n",
    "            s = np.all(Bv[:, idxs], axis=1).mean()\n",
    "            if s >= min_support:\n",
    "                new_prev.append((iset, idxs))\n",
    "                freq.append({\"itemset\": iset, \"support\": float(s)})\n",
    "        prev = new_prev\n",
    "        k += 1\n",
    "\n",
    "    out = []\n",
    "    for rec in freq:\n",
    "        out.append({\"itemset\": tuple(sorted(list(rec[\"itemset\"]))), \"support\": rec[\"support\"]})\n",
    "    return pd.DataFrame(out).sort_values(\"support\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def rules_from_itemsets_fallback(freq_df: pd.DataFrame, min_conf=0.3):\n",
    "    \"\"\"Generate rules with columns: antecedents, consequents, support, confidence, lift.\"\"\"\n",
    "    sup = {frozenset(t): s for t, s in zip(freq_df[\"itemset\"].map(frozenset), freq_df[\"support\"])}\n",
    "    rules = []\n",
    "    for iset_tup, sXY in zip(freq_df[\"itemset\"], freq_df[\"support\"]):\n",
    "        XU = frozenset(iset_tup)\n",
    "        if len(XU) < 2:\n",
    "            continue\n",
    "        items = list(XU)\n",
    "        for r in range(1, len(items)):\n",
    "            for A in itertools.combinations(items, r):\n",
    "                A = frozenset(A)\n",
    "                B = XU - A\n",
    "                sX = sup.get(A)\n",
    "                sY = sup.get(B)\n",
    "                if sX is None or sY is None:\n",
    "                    continue\n",
    "                conf = sXY / (sX + 1e-12)\n",
    "                if conf >= min_conf:\n",
    "                    lift = conf / (sY + 1e-12)\n",
    "                    rules.append({\n",
    "                        \"antecedents\": tuple(sorted(list(A))),\n",
    "                        \"consequents\": tuple(sorted(list(B))),\n",
    "                        \"support\": float(sXY),\n",
    "                        \"confidence\": float(conf),\n",
    "                        \"lift\": float(lift),\n",
    "                    })\n",
    "    return pd.DataFrame(rules).sort_values([\"lift\",\"confidence\",\"support\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "def mine_rules_once(X_bool: pd.DataFrame, min_support: float, max_len: int):\n",
    "    \"\"\"Mine itemsets and rules *one time* for a given support.\"\"\"\n",
    "    if USE_MLXTEND:\n",
    "        df_bin = X_bool.astype(int)\n",
    "        freq = apriori(df_bin, min_support=min_support, use_colnames=True, max_len=max_len)\n",
    "        # Standardize to 'itemset' column (tuple)\n",
    "        freq_std = freq.copy()\n",
    "        freq_std[\"itemset\"] = freq_std[\"itemsets\"].apply(lambda s: tuple(sorted(list(s))))\n",
    "        freq_std = freq_std.drop(columns=[\"itemsets\"]).rename(columns={\"support\": \"support\"})\n",
    "        # Rules (we'll filter later)\n",
    "        rules = association_rules(freq, metric=\"confidence\", min_threshold=0.0)\n",
    "        rules = rules[[\"antecedents\",\"consequents\",\"support\",\"confidence\",\"lift\"]].copy()\n",
    "        rules[\"antecedents\"] = rules[\"antecedents\"].apply(lambda s: tuple(sorted(list(s))))\n",
    "        rules[\"consequents\"] = rules[\"consequents\"].apply(lambda s: tuple(sorted(list(s))))\n",
    "        return freq_std, rules\n",
    "    else:\n",
    "        freq_std = apriori_fallback_bool(X_bool, min_support=min_support, max_len=max_len)\n",
    "        rules = rules_from_itemsets_fallback(freq_std, min_conf=0.0)\n",
    "        return freq_std, rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f7eeaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] min_support=0.080 → 13 high-confidence rules\n",
      "[INFO] Frequent itemsets: 66 (at min_support=0.080)\n",
      "[INFO] All-rules table:   190 rows\n",
      "[INFO] High-conf (≥0.70, lift≥1.50): 13 rows\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 7) High-confidence search (conf ≥ 0.80, lift ≥ 1.50)\n",
    "# -----------------------------\n",
    "def mine_high_confidence_rules(\n",
    "    X_bool: pd.DataFrame,\n",
    "    min_support_grid=MIN_SUPPORT_GRID,\n",
    "    max_len=MAX_LEN,\n",
    "    min_conf=MIN_CONFIDENCE_TARGET,\n",
    "    min_lift=MIN_LIFT_TARGET,\n",
    "    top_k=25\n",
    "):\n",
    "    tried = []\n",
    "    chosen_freq = None\n",
    "    chosen_rules = None\n",
    "    chosen_support = None\n",
    "\n",
    "    for ms in min_support_grid:\n",
    "        freq, rules = mine_rules_once(X_bool, min_support=ms, max_len=max_len)\n",
    "        if freq.empty or rules.empty:\n",
    "            tried.append((ms, len(freq), 0))\n",
    "            continue\n",
    "\n",
    "        # Filter to interpretable rules\n",
    "        rules_f = rules.loc[\n",
    "            (rules[\"confidence\"] >= min_conf) &\n",
    "            (rules[\"lift\"]       >= min_lift) &\n",
    "            (rules[\"antecedents\"].apply(lambda s: len(s) >= 1)) &\n",
    "            (rules[\"consequents\"].apply(lambda s: len(s) == 1))\n",
    "        ].copy()\n",
    "\n",
    "        if rules_f.empty:\n",
    "            tried.append((ms, len(freq), 0))\n",
    "            continue\n",
    "\n",
    "        # Baseline prevalence (support of 1-item consequent)\n",
    "        one_item_sup = {}\n",
    "        for it, sup in zip(freq[\"itemset\"], freq[\"support\"]):\n",
    "            if len(it) == 1:\n",
    "                one_item_sup[list(it)[0]] = float(sup)\n",
    "\n",
    "        rules_f[\"conseq\"] = rules_f[\"consequents\"].apply(lambda s: list(s)[0])\n",
    "        rules_f[\"baseline_support\"] = rules_f[\"conseq\"].map(one_item_sup).astype(float)\n",
    "        rules_f[\"abs_risk_increase\"] = rules_f[\"confidence\"] - rules_f[\"baseline_support\"]\n",
    "\n",
    "        rules_f = rules_f.sort_values([\"confidence\",\"lift\",\"support\"], ascending=False).head(top_k).reset_index(drop=True)\n",
    "\n",
    "        chosen_freq = freq\n",
    "        chosen_rules = rules_f\n",
    "        chosen_support = ms\n",
    "        print(f\"[OK] min_support={ms:.3f} → {len(rules_f)} high-confidence rules\")\n",
    "        break\n",
    "\n",
    "    if chosen_rules is None:\n",
    "        print(f\"[WARN] No rules at confidence ≥ {min_conf:.2f} & lift ≥ {min_lift:.2f}. \"\n",
    "              f\"Consider lowering min_support or raising max_len.\")\n",
    "        print(\"Tried:\", tried)\n",
    "        # Fall back to the weakest grid point for general rules (no high-conf filter), so you still get outputs\n",
    "        ms = min_support_grid[-1]\n",
    "        freq, rules = mine_rules_once(X_bool, min_support=ms, max_len=max_len)\n",
    "        rules_all = rules.copy()\n",
    "        rules_all = rules_all.sort_values([\"lift\",\"confidence\",\"support\"], ascending=False)\n",
    "        return ms, freq, rules_all, pd.DataFrame()\n",
    "\n",
    "    return chosen_support, chosen_freq, None, chosen_rules\n",
    "\n",
    "chosen_ms, freq_used, rules_all_fallback, rules_80 = mine_high_confidence_rules(\n",
    "    X_bool,\n",
    "    min_support_grid=MIN_SUPPORT_GRID,\n",
    "    max_len=MAX_LEN,\n",
    "    min_conf=MIN_CONFIDENCE_TARGET,\n",
    "    min_lift=MIN_LIFT_TARGET,\n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "# If we found high-confidence rules, also produce a \"general\" rules table (for context)\n",
    "if rules_80 is not None and not rules_80.empty:\n",
    "    # Mine once at chosen support and keep all (for reference)\n",
    "    _, _, _, _rules80_tmp = chosen_ms, freq_used, None, rules_80\n",
    "    # For \"all rules\", we can re-mine and keep everything, then pretty-print/save separately\n",
    "    freq_all, rules_all = mine_rules_once(X_bool, min_support=chosen_ms, max_len=MAX_LEN)\n",
    "    rules_all = rules_all.sort_values([\"lift\",\"confidence\",\"support\"], ascending=False).reset_index(drop=True)\n",
    "else:\n",
    "    # No high-confidence set; keep whatever we had as the \"all rules\"\n",
    "    freq_all, rules_all = freq_used, rules_all_fallback\n",
    "\n",
    "print(f\"[INFO] Frequent itemsets: {len(freq_used)} (at min_support={chosen_ms:.3f})\")\n",
    "print(f\"[INFO] All-rules table:   {len(rules_all)} rows\")\n",
    "print(f\"[INFO] High-conf (≥{MIN_CONFIDENCE_TARGET:.2f}, lift≥{MIN_LIFT_TARGET:.2f}): \"\n",
    "      f\"{0 if rules_80 is None else len(rules_80)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e127cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top high-confidence rules (conf ≥ 0.70, lift ≥ 1.50) ===\n",
      "                                    antecedents             consequents support confidence lift baseline_support abs_risk_increase\n",
      "         Chronic_Kidney_Disease, Hyperlipidemia            Hypertension   0.108      0.965 1.54            0.626             0.340\n",
      "Chronic_Kidney_Disease, Coronary_Artery_Disease            Hypertension   0.083      0.958 1.53            0.626             0.333\n",
      "                  Heart_Failure, Hyperlipidemia            Hypertension   0.111      0.940 1.50            0.626             0.315\n",
      "              Coronary_Artery_Disease, Diabetes          Hyperlipidemia   0.087      0.808 1.78            0.454             0.354\n",
      "          Coronary_Artery_Disease, Hypertension          Hyperlipidemia   0.173      0.781 1.72            0.454             0.327\n",
      "                Anemia, Coronary_Artery_Disease          Hyperlipidemia   0.104      0.766 1.69            0.454             0.312\n",
      "         Coronary_Artery_Disease, Heart_Failure          Hyperlipidemia   0.084      0.756 1.67            0.454             0.302\n",
      "                         Diabetes, Hypertension          Hyperlipidemia   0.161      0.740 1.63            0.454             0.286\n",
      "                               Anemia, Diabetes          Hyperlipidemia   0.091      0.735 1.62            0.454             0.281\n",
      "                        Coronary_Artery_Disease          Hyperlipidemia   0.189      0.730 1.61            0.454             0.276\n",
      "                    Heart_Failure, Hypertension          Hyperlipidemia   0.111      0.723 1.59            0.454             0.269\n",
      "                  Heart_Failure, Hyperlipidemia Coronary_Artery_Disease   0.084      0.714 2.76            0.259             0.455\n",
      "           Chronic_Kidney_Disease, Hypertension          Hyperlipidemia   0.108      0.713 1.57            0.454             0.259\n",
      "\n",
      "=== Saved ===\n",
      "{'assoc_transactions.csv': 'D:\\\\HealthAI Project\\\\assoc_artifacts\\\\assoc_transactions.csv', 'frequent_itemsets.csv': 'D:\\\\HealthAI Project\\\\assoc_artifacts\\\\frequent_itemsets.csv', 'association_rules_all.csv': 'D:\\\\HealthAI Project\\\\assoc_artifacts\\\\association_rules_all.csv', 'high_confidence_rules.csv': 'D:\\\\HealthAI Project\\\\assoc_artifacts\\\\high_confidence_rules.csv', 'summary.json': 'D:\\\\HealthAI Project\\\\assoc_artifacts\\\\summary.json'}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 8) Pretty print & save\n",
    "# -----------------------------\n",
    "def tup2str(s): return \", \".join(sorted(list(s)))\n",
    "\n",
    "# Save frequent itemsets\n",
    "freq_out = freq_used.copy()\n",
    "if \"itemset\" in freq_out.columns:\n",
    "    freq_out[\"itemset\"] = freq_out[\"itemset\"].apply(tup2str)\n",
    "freq_out.rename(columns={\"support\": \"support\"}, inplace=True)\n",
    "freq_out.to_csv(ART/\"frequent_itemsets.csv\", index=False)\n",
    "\n",
    "# Save all rules (context)\n",
    "rules_all_out = rules_all.copy()\n",
    "rules_all_out[\"antecedents\"] = rules_all_out[\"antecedents\"].apply(tup2str)\n",
    "rules_all_out[\"consequents\"] = rules_all_out[\"consequents\"].apply(tup2str)\n",
    "rules_all_out.to_csv(ART/\"association_rules_all.csv\", index=False)\n",
    "\n",
    "# Save high-confidence rules\n",
    "if rules_80 is not None and not rules_80.empty:\n",
    "    hc = rules_80.copy()\n",
    "    hc[\"antecedents\"] = hc[\"antecedents\"].apply(tup2str)\n",
    "    hc[\"consequents\"] = hc[\"consequents\"].apply(tup2str)\n",
    "    # order columns nicely\n",
    "    cols = [\"antecedents\",\"consequents\",\"support\",\"confidence\",\"lift\",\"baseline_support\",\"abs_risk_increase\"]\n",
    "    hc = hc[cols]\n",
    "    hc.to_csv(ART/\"high_confidence_rules.csv\", index=False)\n",
    "\n",
    "    print(\"\\n=== Top high-confidence rules (conf ≥ {:.2f}, lift ≥ {:.2f}) ===\".format(\n",
    "        MIN_CONFIDENCE_TARGET, MIN_LIFT_TARGET))\n",
    "    print(hc.head(25).to_string(index=False,\n",
    "                                formatters={\n",
    "                                    \"support\": \"{:.3f}\".format,\n",
    "                                    \"confidence\": \"{:.3f}\".format,\n",
    "                                    \"lift\": \"{:.2f}\".format,\n",
    "                                    \"baseline_support\": \"{:.3f}\".format,\n",
    "                                    \"abs_risk_increase\": \"{:.3f}\".format\n",
    "                                }))\n",
    "else:\n",
    "    print(\"\\n[INFO] No high-confidence rules met the thresholds; \"\n",
    "          \"see association_rules_all.csv for the best available rules.\")\n",
    "\n",
    "# Summary JSON\n",
    "summary = {\n",
    "    \"data_dir_used\": str(Path(DATA_DIR).resolve()),\n",
    "    \"diagnoses_csv_used\": str(Path(DIAG_PATH).resolve()),\n",
    "    \"n_patients_baskets\": int(N),\n",
    "    \"n_items\": int(len(items)),\n",
    "    \"min_support_grid\": list(MIN_SUPPORT_GRID),\n",
    "    \"chosen_min_support\": float(chosen_ms),\n",
    "    \"min_confidence_target\": float(MIN_CONFIDENCE_TARGET),\n",
    "    \"min_lift_target\": float(MIN_LIFT_TARGET),\n",
    "    \"max_len\": int(MAX_LEN),\n",
    "    \"n_frequent_itemsets\": int(len(freq_used)),\n",
    "    \"n_rules_all\": int(len(rules_all)),\n",
    "    \"n_rules_high_conf\": 0 if (rules_80 is None) else int(len(rules_80)),\n",
    "}\n",
    "with open(ART/\"summary.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n=== Saved ===\")\n",
    "print({k: str((ART/k).resolve()) for k in [\n",
    "    \"assoc_transactions.csv\",\n",
    "    \"frequent_itemsets.csv\",\n",
    "    \"association_rules_all.csv\",\n",
    "    \"Assoc_high_confidence_rules.csv\",\n",
    "    \"summary.json\"\n",
    "]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
