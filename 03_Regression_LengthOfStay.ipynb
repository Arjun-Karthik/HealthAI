{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 - Length Of Stay (Regression)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "683446d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, re, json, time, warnings, pickle\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.set_printoptions(suppress=True)\n",
        "pd.set_option(\"display.max_columns\", 120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "79ced090",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================\n",
        "# Config / Paths\n",
        "# ======================\n",
        "SEED = 42\n",
        "rng = np.random.default_rng(SEED)\n",
        "DATA_DIR = r\"D:/HealthAI Project/data\"  # <- change if needed\n",
        "ART = Path(\"./Models/los_artifacts\"); ART.mkdir(parents=True, exist_ok=True)\n",
        "MODELS_DIR = Path(\"./Models/LOS_Model\"); MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ======================\n",
        "# Helpers\n",
        "# ======================\n",
        "def find_one_any(base_dir, basenames):\n",
        "    want = {b.lower() for b in basenames}\n",
        "    for root, _, files in os.walk(base_dir):\n",
        "        for f in files:\n",
        "            if f.lower() in want:\n",
        "                return os.path.join(root, f)\n",
        "    return None\n",
        "\n",
        "def read_csv_auto(path):\n",
        "    if path is None: return None\n",
        "    comp = \"gzip\" if str(path).lower().endswith(\".gz\") else None\n",
        "    for enc in [\"utf-8\",\"ISO-8859-1\",\"cp1252\",\"latin1\"]:\n",
        "        try:\n",
        "            return pd.read_csv(path, compression=comp, low_memory=False, encoding=enc)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return pd.read_csv(path, compression=comp, low_memory=False)\n",
        "\n",
        "def print_header(title):\n",
        "    print(\"\\n\" + \"=\"*len(title)); print(title); print(\"=\"*len(title))\n",
        "\n",
        "def to_int_id(series):\n",
        "    \"\"\"Coerce an ID column to pandas nullable Int64 (handles strings/NaN safely).\"\"\"\n",
        "    return pd.to_numeric(series, errors=\"coerce\").astype(\"Int64\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b82cf7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================\n",
        "# Locate CSVs\n",
        "# ======================\n",
        "admissions_p = find_one_any(DATA_DIR, [\"admissions.csv\"])\n",
        "patients_p   = find_one_any(DATA_DIR, [\"patients.csv\"])\n",
        "diagn_p      = find_one_any(DATA_DIR, [\"diagnoses_icd.csv\", \"diagnoses.csv\"])\n",
        "vitals_p     = find_one_any(DATA_DIR, [\"vitalsign.csv\", \"vitalsigns.csv\"])\n",
        "\n",
        "print_header(\"Files detected\")\n",
        "print({\n",
        "    \"admissions\": admissions_p,\n",
        "    \"patients\": patients_p,\n",
        "    \"diagnoses_icd\": diagn_p,\n",
        "    \"vitalsign\": vitals_p\n",
        "})\n",
        "\n",
        "if admissions_p is None:\n",
        "    raise FileNotFoundError(\"admissions.csv not found under DATA_DIR — required for LOS labels.\")\n",
        "\n",
        "# ======================\n",
        "# Load & normalize\n",
        "# ======================\n",
        "adm = read_csv_auto(admissions_p)\n",
        "pat = read_csv_auto(patients_p) if patients_p else None\n",
        "diag = read_csv_auto(diagn_p) if diagn_p else None\n",
        "vit = read_csv_auto(vitals_p) if vitals_p else None\n",
        "\n",
        "for df in [adm, pat, diag, vit]:\n",
        "    if df is not None:\n",
        "        df.columns = [c.lower() for c in df.columns]\n",
        "        if \"hadm_id\" in df.columns: df[\"hadm_id\"] = to_int_id(df[\"hadm_id\"])\n",
        "        if \"subject_id\" in df.columns: df[\"subject_id\"] = to_int_id(df[\"subject_id\"])\n",
        "\n",
        "# LOS label in days (+ unit guard)\n",
        "for c in (\"admittime\",\"dischtime\",\"deathtime\"):\n",
        "    if c in adm.columns:\n",
        "        adm[c] = pd.to_datetime(adm[c], errors=\"coerce\")\n",
        "\n",
        "if \"admittime\" not in adm.columns or \"dischtime\" not in adm.columns:\n",
        "    raise RuntimeError(\"Expected admittime / dischtime in admissions.csv\")\n",
        "\n",
        "los_raw_days = (adm[\"dischtime\"] - adm[\"admittime\"]).dt.total_seconds()/86400.0\n",
        "los_raw_days = pd.to_numeric(los_raw_days, errors=\"coerce\")\n",
        "\n",
        "# Auto unit guard: if median looks like hours (>> 60), convert to days\n",
        "med = np.nanmedian(los_raw_days)\n",
        "if med is not None and np.isfinite(med) and med > 60:\n",
        "    los_raw_days = los_raw_days / 24.0\n",
        "\n",
        "adm[\"los_days\"] = los_raw_days\n",
        "adm = adm.dropna(subset=[\"los_days\"])\n",
        "adm = adm[adm[\"los_days\"] >= 0.0]\n",
        "\n",
        "# Robust cap (handles extreme outliers)\n",
        "cap_q = 0.95\n",
        "clip_max = float(adm[\"los_days\"].quantile(cap_q))\n",
        "adm[\"los_days\"] = adm[\"los_days\"].clip(0, clip_max)\n",
        "\n",
        "print_header(\"LOS label stats (days, after cap)\")\n",
        "print({\n",
        "    \"n\": int(adm[\"los_days\"].shape[0]),\n",
        "    \"mean\": float(adm[\"los_days\"].mean()),\n",
        "    \"std\": float(adm[\"los_days\"].std(ddof=0)),\n",
        "    \"median\": float(adm[\"los_days\"].median()),\n",
        "    f\"p{int(cap_q*100)}\": clip_max,\n",
        "    \"max\": float(adm[\"los_days\"].max())\n",
        "})\n",
        "\n",
        "# Age/sex\n",
        "def compute_age_at_admit(adm_df, pat_df):\n",
        "    if pat_df is None or \"subject_id\" not in adm_df.columns:\n",
        "        return pd.Series(np.nan, index=adm_df.index)\n",
        "    pm = pat_df.set_index(\"subject_id\")\n",
        "    y = pd.to_datetime(adm_df[\"admittime\"], errors=\"coerce\").dt.year\n",
        "    age = pd.Series(np.nan, index=adm_df.index, dtype=float)\n",
        "    if {\"anchor_year\",\"anchor_age\"}.issubset(pm.columns):\n",
        "        ay = pd.to_numeric(adm_df[\"subject_id\"].map(pm[\"anchor_year\"]), errors=\"coerce\")\n",
        "        aa = pd.to_numeric(adm_df[\"subject_id\"].map(pm[\"anchor_age\"]),  errors=\"coerce\")\n",
        "        age = (aa + (y - ay)).astype(float)\n",
        "    if \"dob\" in pm.columns:\n",
        "        dob = pd.to_datetime(adm_df[\"subject_id\"].map(pm[\"dob\"]), errors=\"coerce\")\n",
        "        age_from_dob = (adm_df[\"admittime\"] - dob).dt.total_seconds()/(365.25*24*3600)\n",
        "        age = age.where(age.notna(), age_from_dob)\n",
        "    return age.clip(lower=0, upper=120)\n",
        "\n",
        "adm[\"age_at_admit\"] = compute_age_at_admit(adm, pat)\n",
        "if pat is not None:\n",
        "    if \"gender\" in pat.columns:\n",
        "        adm[\"sex\"] = adm[\"subject_id\"].map(pat.set_index(\"subject_id\")[\"gender\"])\n",
        "    elif \"sex\" in pat.columns:\n",
        "        adm[\"sex\"] = adm[\"subject_id\"].map(pat.set_index(\"subject_id\")[\"sex\"])\n",
        "else:\n",
        "    adm[\"sex\"] = np.nan\n",
        "\n",
        "# Simple comorb from diagnoses (optional)\n",
        "def icd_to_flags(code, version):\n",
        "    if not isinstance(code, str): code = str(code)\n",
        "    c = code.strip().upper().replace(\".\",\"\")\n",
        "    out = {}\n",
        "    def add(k): out[k]=1\n",
        "    if pd.isna(version): version = 10 if re.match(r\"^[A-Z]\", c) else 9\n",
        "    if version == 9 or (c[:1].isdigit() and len(c)>=3):\n",
        "        if c.startswith(\"250\"): add(\"comor_diabetes\")\n",
        "        try:\n",
        "            p3 = float(c[:3])\n",
        "            if 401 <= p3 < 406: add(\"comor_htn\")\n",
        "            if 410 <= p3 < 415: add(\"comor_cad\")\n",
        "        except Exception: pass\n",
        "        if c.startswith(\"428\"): add(\"comor_hf\")\n",
        "        if c.startswith(\"585\"): add(\"comor_ckd\")\n",
        "        if c.startswith(\"493\"): add(\"comor_asthma\")\n",
        "        if c.startswith(\"272\"): add(\"comor_lipids\")\n",
        "        if c.startswith(\"278\"): add(\"comor_obesity\")\n",
        "    else:\n",
        "        if c.startswith((\"E10\",\"E11\",\"E12\",\"E13\",\"E14\")): add(\"comor_diabetes\")\n",
        "        if c.startswith((\"I10\",\"I11\",\"I12\",\"I13\",\"I15\",\"I16\")): add(\"comor_htn\")\n",
        "        if c.startswith((\"I20\",\"I21\",\"I22\",\"I23\",\"I24\",\"I25\")): add(\"comor_cad\")\n",
        "        if c.startswith(\"I50\"): add(\"comor_hf\")\n",
        "        if c.startswith(\"N18\"): add(\"comor_ckd\")\n",
        "        if c.startswith(\"J45\"): add(\"comor_asthma\")\n",
        "        if c.startswith(\"E78\"): add(\"comor_lipids\")\n",
        "        if c.startswith(\"E66\"): add(\"comor_obesity\")\n",
        "    return out\n",
        "\n",
        "comorb = None\n",
        "if diag is not None and {\"hadm_id\",\"icd_code\"}.issubset(diag.columns):\n",
        "    if \"icd_version\" in diag.columns:\n",
        "        diag[\"icd_version\"] = pd.to_numeric(diag[\"icd_version\"], errors=\"coerce\")\n",
        "    else:\n",
        "        diag[\"icd_version\"] = np.nan\n",
        "    rows = []\n",
        "    for hadm, g in diag.groupby(\"hadm_id\", dropna=True):\n",
        "        flags = {}\n",
        "        for _, r in g.iterrows():\n",
        "            flags.update(icd_to_flags(r[\"icd_code\"], r.get(\"icd_version\", np.nan)))\n",
        "        if flags:\n",
        "            flags[\"hadm_id\"] = hadm\n",
        "            flags[\"comor_count\"] = sum(v for k,v in flags.items() if k.startswith(\"comor_\"))\n",
        "            rows.append(flags)\n",
        "    if rows:\n",
        "        comorb = pd.DataFrame(rows).fillna(0)\n",
        "        comorb[\"hadm_id\"] = to_int_id(comorb[\"hadm_id\"])\n",
        "        for c in [c for c in comorb.columns if c.startswith(\"comor_\")]:\n",
        "            comorb[c] = pd.to_numeric(comorb[c], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "# ======================\n",
        "# Assemble tabular dataset + features\n",
        "# ======================\n",
        "keep_cols = [\"subject_id\",\"hadm_id\",\"admittime\",\"los_days\",\"age_at_admit\",\"sex\",\n",
        "             \"admission_type\",\"admission_location\",\"discharge_location\",\"insurance\",\"language\"]\n",
        "for c in keep_cols:\n",
        "    if c not in adm.columns: adm[c] = np.nan\n",
        "adm[\"subject_id\"] = to_int_id(adm[\"subject_id\"])\n",
        "adm[\"hadm_id\"]    = to_int_id(adm[\"hadm_id\"])\n",
        "\n",
        "base = adm[keep_cols].copy()\n",
        "\n",
        "# Time features from admittime\n",
        "base[\"dow\"]       = pd.to_datetime(base[\"admittime\"], errors=\"coerce\").dt.dayofweek\n",
        "base[\"month\"]     = pd.to_datetime(base[\"admittime\"], errors=\"coerce\").dt.month\n",
        "base[\"hour\"]      = pd.to_datetime(base[\"admittime\"], errors=\"coerce\").dt.hour\n",
        "base[\"is_weekend\"]= base[\"dow\"].isin([5,6]).astype(float)\n",
        "\n",
        "# Emergency flag\n",
        "def is_emerg(x):\n",
        "    x = str(x).lower()\n",
        "    return int((\"emerg\" in x) or (x in {\"er\",\"ed\"}))\n",
        "base[\"is_emergency\"] = base[\"admission_type\"].apply(is_emerg) if \"admission_type\" in base.columns else 0\n",
        "\n",
        "if comorb is not None:\n",
        "    comorb[\"hadm_id\"] = to_int_id(comorb[\"hadm_id\"])\n",
        "    base = base.merge(comorb[[\"hadm_id\",\"comor_count\"]], on=\"hadm_id\", how=\"left\")\n",
        "else:\n",
        "    base[\"comor_count\"] = 0\n",
        "\n",
        "base[\"comor_count\"] = pd.to_numeric(base[\"comor_count\"], errors=\"coerce\").fillna(0.0).astype(float)\n",
        "\n",
        "# Categorical cleanup (preserve NaN -> imputer)\n",
        "for c in [\"admission_type\",\"admission_location\",\"discharge_location\",\"insurance\",\"language\",\"sex\"]:\n",
        "    if c in base.columns:\n",
        "        base[c] = base[c].astype(\"object\")\n",
        "        base[c] = base[c].where(~base[c].isna(), other=np.nan)\n",
        "\n",
        "# Remove rows without labels/IDs\n",
        "base = base.dropna(subset=[\"los_days\",\"subject_id\",\"hadm_id\"]).reset_index(drop=True)\n",
        "\n",
        "print_header(\"Dataset snapshot\")\n",
        "print(base.head(3))\n",
        "\n",
        "# ======================\n",
        "# Split by subject_id\n",
        "# ======================\n",
        "subjects = base[\"subject_id\"].dropna().unique()\n",
        "rng.shuffle(subjects)\n",
        "n = len(subjects)\n",
        "tr_ids = set(subjects[:int(0.7*n)])\n",
        "va_ids = set(subjects[int(0.7*n):int(0.85*n)])\n",
        "te_ids = set(subjects[int(0.85*n):])\n",
        "\n",
        "def mask_ids(ids): return base[\"subject_id\"].isin(ids)\n",
        "train_df = base[mask_ids(tr_ids)].copy()\n",
        "val_df   = base[mask_ids(va_ids)].copy()\n",
        "test_df  = base[mask_ids(te_ids)].copy()\n",
        "\n",
        "print_header(\"Split sizes\")\n",
        "print({k: len(v) for k,v in {\"train\":train_df, \"val\":val_df, \"test\":test_df}.items()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff36cf29",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================\n",
        "# Pipelines with IMPUTERS (fixes NaNs)\n",
        "# ======================\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge, HuberRegressor, LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import joblib\n",
        "import inspect\n",
        "\n",
        "# --- version-safe RMSE helper (always returns RMSE) ---\n",
        "def rmse_score(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float)\n",
        "    try:\n",
        "        if 'squared' in inspect.signature(mean_squared_error).parameters:\n",
        "            return float(mean_squared_error(y_true, y_pred))\n",
        "    except Exception:\n",
        "        pass\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "NUM = [\"age_at_admit\",\"comor_count\",\"dow\",\"month\",\"hour\",\"is_weekend\",\"is_emergency\"]\n",
        "CAT = [\"sex\",\"admission_type\",\"admission_location\",\"discharge_location\",\"insurance\",\"language\"]\n",
        "\n",
        "num_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "cat_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", min_frequency=20))\n",
        "])\n",
        "\n",
        "pre = ColumnTransformer([\n",
        "    (\"num\", num_pipe, NUM),\n",
        "    (\"cat\", cat_pipe, CAT)\n",
        "], remainder=\"drop\")\n",
        "\n",
        "def baseline_report(df, train_mean):\n",
        "    y = df[\"los_days\"].values.astype(float)\n",
        "    p = np.full_like(y, float(train_mean), dtype=float)\n",
        "    rmse = float(np.sqrt(np.mean((p-y)**2)))\n",
        "    mae  = float(np.mean(np.abs(p-y)))\n",
        "    r2   = float(1 - np.sum((y-p)**2)/(np.var(y)*len(y)+1e-9))\n",
        "    return {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2}\n",
        "\n",
        "def sanity_rmse_r2(y, p):\n",
        "    y = np.asarray(y, float); p = np.asarray(p, float)\n",
        "    rmse = float(np.sqrt(np.mean((p-y)**2)))\n",
        "    r2   = float(r2_score(y, p))\n",
        "    rmse_base = float(np.sqrt(np.mean((y - y.mean())**2)))\n",
        "    rmse_expected = float(((1 - r2) ** 0.5) * rmse_base)\n",
        "    return {\"rmse\": rmse, \"r2\": r2, \"rmse_baseline\": rmse_base,\n",
        "            \"rmse_expected_from_r2\": rmse_expected,\n",
        "            \"ratio_rmse_over_expected\": (rmse / (rmse_expected + 1e-12))}\n",
        "\n",
        "class CalibratedPipeline:\n",
        "    \"\"\"Wrap a Pipeline and learn a linear post-hoc calibration y ≈ a*pred+b on train.\n",
        "       NOTE: We DO NOT pickle this class. We save only a pure sklearn pipeline later.\n",
        "    \"\"\"\n",
        "    def __init__(self, pipe):\n",
        "        self.pipe = pipe\n",
        "        self.cal = LinearRegression()\n",
        "        self.fitted = False\n",
        "    def fit(self, X, y):\n",
        "        self.pipe.fit(X, y)\n",
        "        p = self.pipe.predict(X).reshape(-1, 1)\n",
        "        self.cal.fit(p, y)\n",
        "        self.fitted = True\n",
        "        return self\n",
        "    def predict(self, X):\n",
        "        p = self.pipe.predict(X).reshape(-1, 1)\n",
        "        return self.cal.predict(p)\n",
        "\n",
        "def fit_and_eval(name, model, train_df, val_df, test_df, calibrate=True):\n",
        "    base_pipe = Pipeline([(\"pre\", pre), (\"mdl\", model)])\n",
        "    pipe = CalibratedPipeline(base_pipe) if calibrate else base_pipe\n",
        "\n",
        "    FEATS = NUM + CAT\n",
        "    Xtr, ytr = train_df[FEATS], train_df[\"los_days\"].astype(float)\n",
        "    Xva, yva = val_df[FEATS],   val_df[\"los_days\"].astype(float)\n",
        "    Xte, yte = test_df[FEATS],  test_df[\"los_days\"].astype(float)\n",
        "\n",
        "    for y_name, yv in [(\"train\", ytr), (\"val\", yva), (\"test\", yte)]:\n",
        "        if not np.all(np.isfinite(yv)):\n",
        "            raise ValueError(f\"Non-finite y in {y_name} set.\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    pipe.fit(Xtr, ytr)\n",
        "    tr_time = time.time()-t0\n",
        "\n",
        "    def report(X, y):\n",
        "        p = pipe.predict(X)\n",
        "        rmse = rmse_score(y, p)\n",
        "        mae  = float(mean_absolute_error(y, p))\n",
        "        r2   = float(r2_score(y, p))\n",
        "        san  = sanity_rmse_r2(y, p)\n",
        "        return {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2, \"sanity\": san}\n",
        "\n",
        "    rep = {\"train\": report(Xtr, ytr), \"val\": report(Xva, yva), \"test\": report(Xte, yte)}\n",
        "    print_header(f\"{name} metrics\")\n",
        "    print(\"Train :\", {k:v for k,v in rep[\"train\"].items() if k!='sanity'})\n",
        "    print(\"Val   :\", {k:v for k,v in rep[\"val\"].items() if k!='sanity'})\n",
        "    print(\"Test  :\", {k:v for k,v in rep[\"test\"].items() if k!='sanity'})\n",
        "\n",
        "    rat = rep[\"val\"][\"sanity\"][\"ratio_rmse_over_expected\"]\n",
        "    if rat > 2.0:\n",
        "        print(f\"[WARN] RMSE appears inflated vs R² on val (ratio={rat:.2f}). \"\n",
        "              f\"Units or a few extreme outliers may still be present.\")\n",
        "\n",
        "    return pipe, rep, tr_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fac5946f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=============================\n",
            "Baseline (train-mean) metrics\n",
            "=============================\n",
            "Val : {'RMSE': 4.0194242709547305, 'MAE': 3.057494173207971, 'R2': -4.8989754345862835e-05}\n",
            "Test: {'RMSE': 4.000061408635235, 'MAE': 3.046565622906377, 'R2': -7.301197698850181e-05}\n",
            "\n",
            "========================\n",
            "Ridge(alpha=3.0) metrics\n",
            "========================\n",
            "Train : {'RMSE': 3.1179297281505662, 'MAE': 2.1838660740025153, 'R2': 0.4035272912651202}\n",
            "Val   : {'RMSE': 3.0895496166451735, 'MAE': 2.161242184535112, 'R2': 0.4091409083001437}\n",
            "Test  : {'RMSE': 3.079397040220502, 'MAE': 2.1635665152188377, 'R2': 0.4073070433112076}\n",
            "\n",
            "==========================\n",
            "Ridge (log-target) metrics\n",
            "==========================\n",
            "Train : {'RMSE': 3.1194015342574053, 'MAE': 2.15671368455773, 'R2': 0.402964033299378}\n",
            "Val   : {'RMSE': 3.087117153027243, 'MAE': 2.1310814022333187, 'R2': 0.41007093217877255}\n",
            "Test  : {'RMSE': 3.08012016753548, 'MAE': 2.1364571612381256, 'R2': 0.40702864933768534}\n",
            "\n",
            "======================\n",
            "HuberRegressor metrics\n",
            "======================\n",
            "Train : {'RMSE': 3.1438072283896976, 'MAE': 2.1748523917227636, 'R2': 0.393585261181262}\n",
            "Val   : {'RMSE': 3.113227610997899, 'MAE': 2.151950950957782, 'R2': 0.40004963590050957}\n",
            "Test  : {'RMSE': 3.102593114810105, 'MAE': 2.1534918782812142, 'R2': 0.3983442951013376}\n",
            "\n",
            "====================\n",
            "RandomForest metrics\n",
            "====================\n",
            "Train : {'RMSE': 1.8837328656079906, 'MAE': 1.3500466817857544, 'R2': 0.7822808841966258}\n",
            "Val   : {'RMSE': 3.177625621825889, 'MAE': 2.226726544856455, 'R2': 0.374972635893974}\n",
            "Test  : {'RMSE': 3.176818733207024, 'MAE': 2.2316965574580405, 'R2': 0.3692122359646992}\n",
            "\n",
            "=============================\n",
            "Best (by Val RMSE): ridge_log\n",
            "=============================\n",
            "Test metrics: {'RMSE': 3.08012016753548, 'MAE': 2.1364571612381256, 'R2': 0.40702864933768534}\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# Baseline\n",
        "# ======================\n",
        "baseline_mean = float(train_df[\"los_days\"].mean())\n",
        "print_header(\"Baseline (train-mean) metrics\")\n",
        "print(\"Val :\", baseline_report(val_df, baseline_mean))\n",
        "print(\"Test:\", baseline_report(test_df, baseline_mean))\n",
        "\n",
        "# ======================\n",
        "# Models\n",
        "# ======================\n",
        "ridge, ridge_rep, _ = fit_and_eval(\"Ridge(alpha=3.0)\", Ridge(alpha=3.0), train_df, val_df, test_df)\n",
        "\n",
        "# ---- FIXED: log-target wraps only the regressor; preprocessing stays outside ----\n",
        "ridge_log = TransformedTargetRegressor(\n",
        "    regressor=Ridge(alpha=3.0), func=np.log1p, inverse_func=np.expm1\n",
        ")\n",
        "ridge_log_m, ridge_log_rep, _ = fit_and_eval(\"Ridge (log-target)\", ridge_log, train_df, val_df, test_df)\n",
        "\n",
        "huber, huber_rep, _ = fit_and_eval(\"HuberRegressor\", HuberRegressor(alpha=1e-4), train_df, val_df, test_df)\n",
        "\n",
        "rf, rf_rep, _ = fit_and_eval(\n",
        "    \"RandomForest\",\n",
        "    RandomForestRegressor(n_estimators=400, max_depth=None, min_samples_leaf=3, n_jobs=-1, random_state=SEED),\n",
        "    train_df, val_df, test_df\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# Select best by Val RMSE\n",
        "# ======================\n",
        "candidates = [\n",
        "    (\"ridge\", ridge, ridge_rep),\n",
        "    (\"ridge_log\", ridge_log_m, ridge_log_rep),\n",
        "    (\"huber\", huber, huber_rep),\n",
        "    (\"rf\", rf, rf_rep)\n",
        "]\n",
        "best_name, best_model, best_rep = min(candidates, key=lambda t: t[2][\"val\"][\"RMSE\"])\n",
        "\n",
        "print_header(f\"Best (by Val RMSE): {best_name}\")\n",
        "print(\"Test metrics:\", {k:v for k,v in best_rep[\"test\"].items() if k!='sanity'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "367dae5a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================\n",
            "Saved inference pipeline\n",
            "========================\n",
            "{'path': 'D:\\\\HealthAI Project\\\\Models\\\\los_inference_pipeline.pkl', 'features': ['age_at_admit', 'comor_count', 'dow', 'month', 'hour', 'is_weekend', 'is_emergency', 'sex', 'admission_type', 'admission_location', 'discharge_location', 'insurance', 'language']}\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# SAVE: production-ready inference pipeline (pure sklearn, picklable)\n",
        "# ======================\n",
        "# We save ONLY a plain sklearn Pipeline: ('pre', ColumnTransformer) + ('model', final estimator).\n",
        "# Do NOT save the custom CalibratedPipeline wrapper.\n",
        "from sklearn.base import RegressorMixin\n",
        "\n",
        "FEATS = NUM + CAT\n",
        "X_train_full = train_df[FEATS].copy()\n",
        "y_train_full = train_df[\"los_days\"].astype(float).values\n",
        "\n",
        "# Build clean inference pipeline\n",
        "inference_pipe = Pipeline([\n",
        "    (\"pre\", pre),\n",
        "    (\"model\", best_model.pipe.steps[-1][1] if isinstance(best_model, CalibratedPipeline) else best_model.steps[-1][1])\n",
        "    if isinstance(best_model, (CalibratedPipeline, Pipeline)) else\n",
        "    (\"model\", best_model)  # fallback if already a regressor\n",
        "])\n",
        "\n",
        "# If the logic above confuses, simplify:\n",
        "if isinstance(best_model, CalibratedPipeline):\n",
        "    # take the underlying sklearn Pipeline and extract the final regressor\n",
        "    final_reg = best_model.pipe.steps[-1][1]\n",
        "    inference_pipe = Pipeline([(\"pre\", pre), (\"model\", final_reg)])\n",
        "elif isinstance(best_model, Pipeline):\n",
        "    final_reg = best_model.steps[-1][1]\n",
        "    inference_pipe = Pipeline([(\"pre\", pre), (\"model\", final_reg)])\n",
        "else:\n",
        "    # plain estimator already\n",
        "    inference_pipe = Pipeline([(\"pre\", pre), (\"model\", best_model)])\n",
        "\n",
        "# Fit on full training data\n",
        "inference_pipe.fit(X_train_full, y_train_full)\n",
        "\n",
        "# Persist model + feature list\n",
        "MODEL_PATH = MODELS_DIR / \"los_inference_pipeline.pkl\"\n",
        "joblib.dump(inference_pipe, MODEL_PATH)\n",
        "with open(MODELS_DIR / \"los_features.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"features\": FEATS}, f)\n",
        "\n",
        "# Smoke test reload -> predict\n",
        "reloaded = joblib.load(MODEL_PATH)\n",
        "_ = reloaded.predict(X_train_full.head(5))\n",
        "print_header(\"Saved inference pipeline\")\n",
        "print({\"path\": str(MODEL_PATH.resolve()), \"features\": FEATS})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2da6daa9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================\n",
        "# (Optional) Save individual training-time wrappers to artifacts (debug only)\n",
        "# ======================\n",
        "with open(ART/\"ridge_model.pkl\",\"wb\")      as f: pickle.dump(ridge, f)\n",
        "with open(ART/\"ridge_log_model.pkl\",\"wb\")  as f: pickle.dump(ridge_log_m, f)\n",
        "with open(ART/\"huber_model.pkl\",\"wb\")      as f: pickle.dump(huber, f)\n",
        "with open(ART/\"rf_model.pkl\",\"wb\")         as f: pickle.dump(rf, f)\n",
        "with open(ART/\"best_model_name.txt\",\"w\",encoding=\"utf-8\") as f: f.write(best_name)\n",
        "with open(ART/\"best_model.pkl\",\"wb\")       as f: pickle.dump(best_model, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ed889d98",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===============\n",
            "Artifacts saved\n",
            "===============\n",
            "{'ridge_model.pkl': 'D:\\\\HealthAI Project\\\\los_artifacts\\\\ridge_model.pkl', 'ridge_log_model.pkl': 'D:\\\\HealthAI Project\\\\los_artifacts\\\\ridge_log_model.pkl', 'huber_model.pkl': 'D:\\\\HealthAI Project\\\\los_artifacts\\\\huber_model.pkl', 'rf_model.pkl': 'D:\\\\HealthAI Project\\\\los_artifacts\\\\rf_model.pkl', 'best_model.pkl': 'D:\\\\HealthAI Project\\\\los_artifacts\\\\best_model.pkl', 'best_model_name.txt': 'D:\\\\HealthAI Project\\\\los_artifacts\\\\best_model_name.txt', 'report.json': 'D:\\\\HealthAI Project\\\\los_artifacts\\\\report.json', 'inference_pipeline.pkl': 'D:\\\\HealthAI Project\\\\Models\\\\los_inference_pipeline.pkl', 'los_features.json': 'D:\\\\HealthAI Project\\\\Models\\\\los_features.json'}\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# Consolidated report (+ sanity)\n",
        "# ======================\n",
        "def pack(rep):\n",
        "    return {k: {mk: mv for mk, mv in v.items() if mk != \"sanity\"} for k, v in rep.items()}\n",
        "report = {\n",
        "    \"label_cap_percentile\": 0.95,\n",
        "    \"label_stats_days\": {\n",
        "        \"mean\": float(adm[\"los_days\"].mean()),\n",
        "        \"std\": float(adm[\"los_days\"].std(ddof=0)),\n",
        "        \"median\": float(adm[\"los_days\"].median()),\n",
        "        \"p95\": float(adm[\"los_days\"].quantile(0.95)),\n",
        "        \"max\": float(adm[\"los_days\"].max())\n",
        "    },\n",
        "    \"baseline\": {\n",
        "        \"val\": baseline_report(val_df, baseline_mean),\n",
        "        \"test\": baseline_report(test_df, baseline_mean)\n",
        "    },\n",
        "    \"ridge\": pack(ridge_rep),\n",
        "    \"ridge_log\": pack(ridge_log_rep),\n",
        "    \"huber\": pack(huber_rep),\n",
        "    \"rf\": pack(rf_rep),\n",
        "    \"best\": {\"name\": best_name, \"test\": {k:v for k,v in best_rep[\"test\"].items() if k!='sanity'}}\n",
        "}\n",
        "\n",
        "with open(ART/\"report.json\",\"w\",encoding=\"utf-8\") as f: json.dump(report, f, indent=2)\n",
        "\n",
        "print_header(\"Artifacts saved\")\n",
        "print({\n",
        "    \"ridge_model.pkl\": str((ART/\"ridge_model.pkl\").resolve()),\n",
        "    \"ridge_log_model.pkl\": str((ART/\"ridge_log_model.pkl\").resolve()),\n",
        "    \"huber_model.pkl\": str((ART/\"huber_model.pkl\").resolve()),\n",
        "    \"rf_model.pkl\": str((ART/\"rf_model.pkl\").resolve()),\n",
        "    \"best_model.pkl\": str((ART/\"best_model.pkl\").resolve()),\n",
        "    \"best_model_name.txt\": str((ART/\"best_model_name.txt\").resolve()),\n",
        "    \"report.json\": str((ART/\"report.json\").resolve()),\n",
        "    \"inference_pipeline.pkl\": str(MODELS_DIR.joinpath(\"los_inference_pipeline.pkl\").resolve()),\n",
        "    \"los_features.json\": str(MODELS_DIR.joinpath(\"los_features.json\").resolve())\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b6ebc4d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=============================================\n",
            "Merging all sources into one prediction table\n",
            "=============================================\n",
            "Merged vitals summary: (0, 1)\n",
            "\n",
            "================================\n",
            "Merged prediction table snapshot\n",
            "================================\n",
            "   subject_id   hadm_id           admittime  dischtime  age_at_admit  \\\n",
            "0    10000032  22595853 2180-05-06 22:23:00        NaN          52.0   \n",
            "1    10000032  22841357 2180-06-26 18:27:00        NaN          52.0   \n",
            "2    10000032  25742920 2180-08-05 23:44:00        NaN          52.0   \n",
            "3    10000032  29079034 2180-07-23 12:35:00        NaN          52.0   \n",
            "4    10000068  25022803 2160-03-03 23:16:00        NaN          19.0   \n",
            "\n",
            "   comor_count  dow  month  hour  is_weekend  is_emergency sex  \\\n",
            "0          0.0    5      5    22         1.0             0   F   \n",
            "1          0.0    0      6    18         0.0             0   F   \n",
            "2          0.0    5      8    23         1.0             0   F   \n",
            "3          0.0    6      7    12         1.0             0   F   \n",
            "4          0.0    0      3    23         0.0             0   F   \n",
            "\n",
            "   admission_type      admission_location discharge_location insurance  \\\n",
            "0          URGENT  TRANSFER FROM HOSPITAL               HOME  Medicaid   \n",
            "1        EW EMER.          EMERGENCY ROOM               HOME  Medicaid   \n",
            "2        EW EMER.          EMERGENCY ROOM            HOSPICE  Medicaid   \n",
            "3        EW EMER.          EMERGENCY ROOM               HOME  Medicaid   \n",
            "4  EU OBSERVATION          EMERGENCY ROOM                NaN       NaN   \n",
            "\n",
            "  language  los_days  \n",
            "0  English  0.786111  \n",
            "1  English  1.015278  \n",
            "2  English  1.754167  \n",
            "3  English  2.222222  \n",
            "4  English  0.298611  \n",
            "[OK] Wrote merged table for prediction → D:\\HealthAI Project\\Models\\los_data_for_prediction.csv\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# Merge all CSVs → single table for prediction\n",
        "# ======================\n",
        "print_header(\"Merging all sources into one prediction table\")\n",
        "\n",
        "# --- 1) Start from base (already includes LOS label, age/sex, time-features, emergency flag, comorb_count) ---\n",
        "merged = base.copy()\n",
        "\n",
        "# --- 2) OPTIONAL: Add vitals 24h summaries per admission if vitals are available ---\n",
        "def summarize_vitals(vit_df: pd.DataFrame, adm_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns per-hadm_id summary of vitals.\n",
        "    If 'charttime' exists, restrict to first 24h since admission.\n",
        "    Otherwise, aggregates across all rows available for that hadm_id.\n",
        "    \"\"\"\n",
        "    v = vit_df.copy()\n",
        "    v.columns = [c.lower() for c in v.columns]\n",
        "    # normalize ids\n",
        "    if \"hadm_id\" not in v.columns:\n",
        "        return pd.DataFrame({\"hadm_id\": pd.Series(dtype=\"Int64\")})  # nothing to merge\n",
        "\n",
        "    v[\"hadm_id\"] = to_int_id(v[\"hadm_id\"])\n",
        "\n",
        "    # Try to standardize a timestamp column\n",
        "    ts_col = None\n",
        "    for cand in (\"charttime\", \"chart_time\", \"measured_time\", \"event_time\"):\n",
        "        if cand in v.columns:\n",
        "            ts_col = cand\n",
        "            break\n",
        "    if ts_col is not None:\n",
        "        v[ts_col] = pd.to_datetime(v[ts_col], errors=\"coerce\")\n",
        "\n",
        "    # Common vital columns if present\n",
        "    maybe_vital_cols = [\n",
        "        # heart/bp\n",
        "        \"heartrate\",\"heart_rate\",\"hr\",\n",
        "        \"sbp\",\"sysbp\",\"systolic_bp\",\"bp_systolic\",\n",
        "        \"dbp\",\"diasbp\",\"diastolic_bp\",\"bp_diastolic\",\n",
        "        \"mbp\",\"meanbp\",\"map\",\n",
        "        # resp/temp/spo2\n",
        "        \"resp_rate\",\"respiratory_rate\",\"rr\",\n",
        "        \"temperature\",\"temp_c\",\"temp_f\",\n",
        "        \"spo2\",\"o2sat\",\"oxygen_saturation\",\n",
        "    ]\n",
        "    present = [c for c in maybe_vital_cols if c in v.columns]\n",
        "\n",
        "    if not present:\n",
        "        # nothing to aggregate\n",
        "        return pd.DataFrame({\"hadm_id\": v[\"hadm_id\"].dropna().unique()})\n",
        "\n",
        "    # Keep rows within first 24h of admission if we can align by time\n",
        "    if ts_col is not None and {\"hadm_id\", \"admittime\"}.issubset(adm_df.columns):\n",
        "        adm_times = adm_df[[\"hadm_id\", \"admittime\"]].dropna().copy()\n",
        "        adm_times[\"hadm_id\"] = to_int_id(adm_times[\"hadm_id\"])\n",
        "        v = v.merge(adm_times, on=\"hadm_id\", how=\"left\")\n",
        "        # Keep only obs in [admittime, admittime+24h]\n",
        "        v = v[(v[ts_col].notna()) & (v[\"admittime\"].notna())]\n",
        "        dt = (v[ts_col] - v[\"admittime\"]).dt.total_seconds() / 3600.0\n",
        "        v = v[(dt >= 0) & (dt <= 24)]\n",
        "\n",
        "    # Aggregate (mean/median) by hadm_id\n",
        "    agg_map = {c: [\"mean\", \"median\"] for c in present}\n",
        "    g = v.groupby(\"hadm_id\")[present].agg(agg_map)\n",
        "    # Flatten MultiIndex columns: e.g., 'heartrate_mean', 'heartrate_median'\n",
        "    g.columns = [f\"{col}_{stat}\" for col, stat in g.columns]\n",
        "    g = g.reset_index()\n",
        "    return g\n",
        "\n",
        "vit_agg = pd.DataFrame({\"hadm_id\": pd.Series(dtype=\"Int64\")})\n",
        "if vit is not None:\n",
        "    try:\n",
        "        vit_agg = summarize_vitals(vit, adm)\n",
        "        merged = merged.merge(vit_agg, on=\"hadm_id\", how=\"left\")\n",
        "        print(f\"Merged vitals summary: {vit_agg.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not summarize/merge vitals: {e}\")\n",
        "\n",
        "# --- 3) (Already done above) comorbidities merged as comor_count in `base` ---\n",
        "\n",
        "# --- 4) Order and type-clean for prediction ---\n",
        "# Keep ID/time columns for traceability, then features your model expects\n",
        "ID_TIME_COLS = [\"subject_id\", \"hadm_id\", \"admittime\", \"dischtime\"]\n",
        "for c in ID_TIME_COLS:\n",
        "    if c not in merged.columns:\n",
        "        merged[c] = np.nan\n",
        "\n",
        "# Model features (must match your training pipeline)\n",
        "FEATS = [\"age_at_admit\",\"comor_count\",\"dow\",\"month\",\"hour\",\"is_weekend\",\"is_emergency\",\n",
        "         \"sex\",\"admission_type\",\"admission_location\",\"discharge_location\",\"insurance\",\"language\"]\n",
        "\n",
        "# Ensure all FEATS exist (add NaN placeholders if missing so imputers can handle them)\n",
        "for c in FEATS:\n",
        "    if c not in merged.columns:\n",
        "        merged[c] = np.nan\n",
        "\n",
        "# Final column order: IDs/times → FEATS → (any vitals summaries if you want to keep them)\n",
        "ordered_cols = ID_TIME_COLS + FEATS + [c for c in merged.columns\n",
        "                                       if c not in ID_TIME_COLS + FEATS + [\"los_days\"]]\n",
        "merged = merged.loc[:, ordered_cols + ([\"los_days\"] if \"los_days\" in merged.columns else [])]\n",
        "\n",
        "# De-duplicate per admission (keep the first row if merges created multiples)\n",
        "merged = merged.sort_values(by=[\"subject_id\",\"hadm_id\",\"admittime\"], na_position=\"last\")\n",
        "merged = merged.drop_duplicates(subset=[\"hadm_id\"], keep=\"first\").reset_index(drop=True)\n",
        "\n",
        "print_header(\"Merged prediction table snapshot\")\n",
        "print(merged.head(5))\n",
        "\n",
        "# Save a clean CSV you can use directly for inference (the model will use FEATS; extra cols are for reference)\n",
        "MERGED_CSV = MODELS_DIR / \"los_test_data.csv\"\n",
        "merged.to_csv(MERGED_CSV, index=False)\n",
        "print(f\"[OK] Wrote merged table for prediction → {MERGED_CSV.resolve()}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
